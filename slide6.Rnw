\documentclass[a4paper]{beamer}

\usepackage[utf8]{inputenc}
\usepackage{url}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}

%\PassOptionsToPackage{pdf}{pstricks} %used for pdflatex
%\usepackage{pstricks,pst-plot,pst-node,pst-func}

\newcommand{\R}{{\bf R}}
\newcommand{\emp}[1]{\textcolor{blue}{#1}}
\newcommand{\fun}[1]{{\tt #1}}
\newcommand{\eex}{\vspace{\baselineskip}}
\newcommand{\var}[1]{{\tt #1}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bx}{\mathbf{x}}

\usetheme{Singapore}% best choice.
 \setbeamercovered{transparent}%

%\usepackage{natbib}

 <<include=FALSE>>=
library(knitr)
opts_chunk$set(
fig.path='figs/', fig.height = 5,cache=TRUE,cache.path="cache8/"
)
@

\title{Parametric models}

\subtitle{Demography and Event History using R \\ UmeÃ¥ 1--12 June 2015}
\date{June 8, 2015}

\begin{document}

\maketitle

\begin{frame}{Parametric models}

In \emp{\tt eha} three kinds of parametric models are available;
\begin{itemize}
\item the \emp{Proportional  Hazards} (\emp{PH}),
\item the \emp{Accelerated Failure
  Time} (\emp{AFT}) and
\item the \emp{discrete
  time proportional hazards}  models.
\end{itemize}

\end{frame}

\begin{frame}{Generating a proportional hazards family}

\begin{itemize}
\item  If \emp{$h_0$} is the hazard function
corresponding to the \emp{generating distribution},
\item  \emp{$h_1$} is a \emp{member of the family} if
\begin{equation*}
  \emp{h_1(t) = c h_0(t)} \quad \text{for some } c > 0, \; \text{and all } t > 0.
\end{equation*}

\end{itemize}

\emp{Note:} The resulting proportional hazards
class of distributions \emp{may or may not} be a \emp{well recognized family} of
distributions.

\end{frame}

\begin{frame}{Parametric distributions in eha}

The parametric distributions that can be used as the \emp{generating
distribution} in the function \emp{\tt phreg} are the

\begin{itemize}
\item \emp{Weibull},
\item \emp{Piecewise constant hazards} (\fun{``pch''}),
\item \emp{Extreme value},
\item \emp{Gompertz},
\item \emph{Lognormal},
\item \emph{Loglogistic}.
\end{itemize}

 The \emp{lognormal} and
\emp{loglogistic} distributions
allow for hazard functions that are first increasing to a maximum and then
decreasing.
\end{frame}

\begin{frame}[fragile]{Some examples}


<<6hazs,echo=FALSE,fig.height=5,message=FALSE>>=
require(eha)
oldpar <- par(mfrow = c(2, 2))
x <- seq(0, 10, length = 1000)
plot(x, hweibull(x, shape = 2, scale = 2), main = "Weibull, p = 2", type = "l",
     ylab = "", xlab = "Time")
plot(x, hweibull(x, shape = 1/2, scale = 2), main = "Weibull, p = 1/2", type = "l",
     ylab = "", xlab = "Time", ylim = c(0, 2))
plot(x, hlnorm(x, shape = 1, scale = 2), main = "Lognormal", type = "l",
     ylab = "", xlab = "Time")
y <- hgompertz(x, scale = 5, shape = 1)
plot(x, y, main = "Gompertz", type = "l",
     ylab = "", xlab = "Time", ylim = c(0, max(y)))
par(oldpar)
@

\end{frame}


\begin{frame}{Two disparate distribution choices}

\begin{itemize}
\item The \emp{Weibull} distribution,

\begin{itemize}
\item One reason for the popularity of
the \emp{Weibull} distribution is that the survivor and hazard functions
both have fairly simple closed forms, and the proportional hazards models
built from a \emp{Weibull} distribution stays within the family of \emp{Weibull}
distributions.
\end{itemize}

\item The \emp{Lognormal} distribution
\begin{itemize}
\item does not have
any of these nice properties. Is \emp{not} a popular choice among PH models.
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}[fragile]{The Weibull distribution}

\begin{equation*}
h(t; (p, \lambda)) = \frac{p}{\lambda}
\biggl(\frac{t}{\lambda}\biggr)^{p-1}, \; t > 0; \; p, \lambda > 0
\end{equation*}

is \emp{closed under PH}:

\begin{equation*}
c \times h(t; (p, \lambda))  = h(t; (p, \lambda^\prime))
\end{equation*}

with
\begin{equation*}
\lambda^\prime = \lambda c^{-\frac{1}{p}}
\end{equation*}

\begin{itemize}
\item for \emp{each fixed $p > 0$}, a \emp{PH family} is
generated by \emp{varying $\lambda$} (the \emp{scale}).
\item two members from the Weibull family,  with
\emp{different values of $p$} (the \emp{shape}), are \emp{not proportional}.
\end{itemize}

\end{frame}


\begin{frame}[fragile]{Weibull PH regression}


The \emp{PH regression} model with a \emp{Weibull} baseline
distribution as obtained by \emp{multiplying by $\exp(\bbeta \bx)$}:
\begin{equation*}
  h(t; \bx, \lambda, p, \bbeta) = \frac{p}{\lambda}
  \biggl(\frac{t}{\lambda}\biggr)^{p-1} e^{\bbeta \bx}, \quad t > 0.
\end{equation*}
The function \emp{\tt phreg} in package \emp{\tt eha} fits these models by default.

\end{frame}

\begin{frame}[fragile]{Crude cumulative hazards}

<<plotsex,fig.height=4>>=
with(oldmort, plot(Surv(enter, exit, event),
                   strata = sex, col = c("blue", "red"),
                   xlab = "Age"))
@

\end{frame}


\begin{frame}[fragile]{The data set, center covariates}
The data set {\tt fert}:
  birth intervals for married women in 19th century Skellefte{\aa}.
  Here only the
  intervals starting with the first birth for each woman are considered.

\scriptsize

<<fert6.12>>=
library(xtable)
require(eha)
data(fert)
f12 <- fert[fert$parity == 1, ]
f12$Y <- Surv(f12$next.ivl, f12$event)
f12$age <- f12$age - 25
f12$year <- f12$year - 1850
head(f12)
@

\end{frame}

\begin{frame}[fragile]{The 'Y' variable}

Some women never got a second child, for instance the first woman ({\tt id
  = 1}) above. Also, note the just created variable {\tt Y}; it is printed
as a vector, with some values having a trailing``$+$''; Those are the
censored observations ({\tt event = 0}). But {\tt Y} is really a matrix, in
this case with two columns. The first column equals {\tt next.ivl} and the
second is {\tt event}.
<<Y12>>=
is.matrix(f12$Y)
dim(f12$Y)
@

\end{frame}

\begin{frame}[fragile]{The Weibull fit}

<<phweib, results='asis'>>=
fit.w <- phreg(Y ~ age + year + ses, data = f12)
xtable(drop1(fit.w, test = "Chisq"))
@

\begin{equation*}
\text{AIC} = 2 \bigl(n - \max(\text{loglik})\bigr), \; n = \text{No.\ of parameters}
\end{equation*}

 (``Akaike Information Criterion'', 1973)

``Smaller is better!''
\end{frame}


\begin{frame}[fragile]{The Weibull fit, time to second birth}

\scriptsize

<<fitw,results='asis'>>=
ltx(fit.w)
kof <- fit.w$coef[1]
@

The estimated coefficient for {\tt age},
\Sexpr{round(kof, digits = 3)}, is negative, \ldots

\end{frame}


\begin{frame}[fragile]{The baseline hazard}
\scriptsize
<<weibfert6,fig.height=3>>=
oldpar <- par(mfrow = c(1, 2))
plot(fit.w, fn = "haz")
plot(fit.w, fn = "cum")
par(oldpar)
@

\end{frame}

\begin{frame}{The Lognormal model}

\begin{itemize}

\item $X$ is \emp{Lognormal} implies \emp{$Y = \log(X)$ is Normal}.


\item Both the
hazard and the survivor functions \emp{lack closed forms}.

\item \emp{Multiplying the hazard function by a constant} not equal to 1
leads to a \emp{non-lognormal} hazard function.

\item So, while we for the \emp{Weibull} family of distributions found subfamilies with
proportional hazards by keeping $p$ fixed and varying $\lambda$, for the
\emp{lognormal} distribution we cannot use the same trick.
\item By
multiplying the hazard function by a constant we arrive at a \emp{three-parameter
family} of distributions.

\end{itemize}

\end{frame}

\begin{frame}[fragile]{Lognormal fit}
\scriptsize
<<phlognorm,results='asis'>>=
fit.lognorm <- phreg(Y ~ age + year + ses, data = f12,
                     dist = "lognormal")
xtable(drop1(fit.lognorm, test = "Chisq"))
@

\end{frame}


\begin{frame}[fragile]{The Lognormal fit}
\scriptsize
<<fitlogist,results='asis'>>=
dr = drop1(fit.lognorm, test = "Chisq")
ltx(fit.lognorm, dr = dr, digits = 4)
@
<<secret16,echo=FALSE>>=
kof <- fit.lognorm$coef[1]
@
The estimated coefficient for {\tt age},
\Sexpr{round(kof, digits = 3)}, is negative, \ldots

\end{frame}

\begin{frame}[fragile]{The figure, Lognormal}
\scriptsize
<<lognormfert6,fig.height=3>>=
oldpar <- par(mfrow = c(1, 2))
plot(fit.lognorm, fn = "haz", main = "Hazard function")
plot(fit.lognorm, fn = "cum", main = "Cumulative hazard function")
par(oldpar)
@

\end{frame}


\begin{frame}{Comparing the Weibull and Lognormal fits}

\begin{itemize}
\item At least one of the fits must be less good (Why?)
\item There are two direct ways
of comparing the fits.
\begin{enumerate}
\item Look at the maximized log likelihoods. For
the \emp{Weibull} fit it is \Sexpr{round(fit.w$loglik[2], digits = 2)} and for
the \emp{lognormal} fit it is
\emp{\Sexpr{round(fit.lognorm$loglik[2], digits = 2)}}. The rule is that the
largest value wins, so the \emp{lognormal} model is the clear winner.
\begin{itemize}
\item Note that
this is \emp{not} a formal test; a likelihood ratio test
would require that the two models are \emp{nested}, but that
is not the case here. This is (here) equivalent to using the \emp{AIC}
as \emp{a
measure of comparison}, because the two models have the same number of
parameters.
\end{itemize}
\item Graphical: Plot the cumulative
hazard functions
against the nonparametric estimate from a Cox
regression
fit, and judge which looks closer.
\end{enumerate}
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Graphical check of fit}
\scriptsize
\begin{itemize}
\item Fit a Cox model with
the same covariates:
<<coxr6>>=
fit.cr <- coxreg(Y ~ age + year + ses, data = f12)
@

\item Then the function {\tt check.dist} (from {\tt eha}) is called. The \emp{Weibull} fit:

<<drycompw,fig.height=4>>=
check.dist(fit.cr, fit.w)
@

\end{itemize}

This fit looks very poor.
\end{frame}


\begin{frame}[fragile]{Shorter follow-up time}

\scriptsize
Note below that after calling {\tt age.window},
$Y$ \emp{must} be recreated! This is the danger with this ``lazy'' approach.
<<trunc6f12cens,results='asis'>>=
f12$enter <- rep(0, NROW(f12))
f12.cens <- age.window(f12, c(0, 12),
                       surv = c("enter", "next.ivl", "event"))
f12.cens$Y <- Surv(f12.cens$enter, f12.cens$next.ivl,
                   f12.cens$event)
fit.wc <- phreg(Y ~ age + year + ses, data = f12.cens)
fit.c <- coxreg(Y ~ age + year + ses, data = f12.cens)
ltx(fit.wc)
@
\end{frame}

\begin{frame}[fragile]{Graphical check, Weibull, 0--12 years}

<<drycompwcens>>=
check.dist(fit.c, fit.wc)
@

This wasn't much better.

\end{frame}

\begin{frame}[fragile]{The lognormal model}
\scriptsize
<<trunc6f12cens.ln>>=
fit.lnc <- phreg(Y ~ age + year + ses, data = f12.cens,
                 dist = "lognormal")
dr <- drop1(fit.lnc, test = "Chisq")
fit.lnc
@

\end{frame}

\begin{frame}[fragile]{Graphical check, Lognormal}

<<drycomplncens>>=
check.dist(fit.c, fit.lnc)
@

\end{frame}

\begin{frame}[fragile]{Why do we get so lousy fits?}

\begin{itemize}
\item This distribution has
features that the ordinary parametric distributions cannot cope with: Its
hazard function starts off \emp{being zero for almost one year}.
\item There is a positive probability of not getting a second child.
\item Maybe try a \emp{cure} model? Something like

\begin{equation*}
P(T > t) = P(\text{infertile}) + P(\text{fertile}) P(T > t \mid
  \text{fertile})
\end{equation*}

\end{itemize}

\end{frame}

\end{document}

\begin{frame}{The piecewise constant hazards (pch) model}

The pch distribution is flexible because you can add as many parameters as
you want. We will try it on the birth interval data. We start off with just
four intervals, and the non-censored data set.
<<pch6fert>>=
fit.pch <- phreg(Surv(next.ivl, event) ~ age + year + ses,
                 data = f12, dist = "pch", cuts = c(4, 8, 12))
fit.c <- coxreg(Surv(next.ivl, event) ~ age + year + ses,
                data = f12)
fit.pch
@
Then we check the fit in Figure~\ref{fig:pchcheck6}.
\begin{figure}[ht!]
<<drycomppchcens>>=
check.dist(fit.c, fit.pch)
@
\caption{Check of the \emp{pch} model, uncensored birth intervals.}
\label{fig:pchcheck6}
\end{figure}
This is not good enough. We need shorter intervals close to zero, so
<<pchfert2>>=
fit.pch <- phreg(Surv(next.ivl, event) ~ age + year + ses,
                 data = f12, dist = "pch", cuts = 1:13)
fit.pch
@
This gives us 14 intervals with constant baseline hazard, i.e., 14
parameters to estimate only for the baseline hazard function! But the fit
is good, see Figure~\ref{fig:pch13check6}. In fact, it is almost perfect!
\begin{figure}[ht!]
<<drycomppch13cens>>=
check.dist(fit.c, fit.pch)
@
\caption[Check of the \emp{pch} model with 13 constant hazard intervals]{Check of the \emp{pch} model with thirteen constant hazard intervals,
  uncensored birth intervals.}
\label{fig:pch13check6}
\end{figure}

One of the advantages with parametric models is that it is easy to study
and plot the hazard function, and not only the cumulative hazards function,
which is the dominant tool for (graphical) analysis in the case of the
nonparametric model of Cox regression\index{Cox regression}. For the
piecewise constant model
just fitted we get the estimated hazard function in Figure~\ref{fig:pchhaz6}.
\begin{figure}[ht!]
<<pchhaz6>>=
plot(fit.pch, fn = "haz")
@
\caption[Estimated hazard function for birth intervals,
  \emp{pch}]{The estimated hazard function for the length of birth intervals,
  piecewise constant hazards distribution.}
\label{fig:pchhaz6}
\end{figure}
The peak is reached during the third year of waiting, and after that the
intensity of giving birth drops fast, and it becomes zero after 12 years.

What are the implications for the estimates of the regression parameters of
the different choices of parametric model? Let us compare the regression
parameter estimates only for the three distributional assumptions, \emp{Weibull},
\emp{lognormal}, and \emp{pch}. See Table~\ref{tab:threecomp6}
\begin{table}[ht!]
<<threecomp6,echo=FALSE>>=
##require(xtable)
##fit.c$coef
##fit.w$coef
##fit.lognorm$coef
##fit.pch$coef
ta <- cbind(fit.c$coef[1:5], fit.pch$coef[1:5], fit.w$coef[1:5], fit.lognorm$coef[1:5])
colnames(ta) <- c("Cox", "Pch", "Weibull", "Lognormal")
ta <- round(exp(ta), digits = 3)
print(ta)
@
\caption[Distribution assumptions for the birth intervals
  data]{Parameter estimates under different distribution assumptions for
    the birth intervals data.}
\label{tab:threecomp6}
\end{table}
We can see that the differences are not large. However, the two assumptions
closest to the ``truth'', the Cox model and the Pch model, are very close,
while the other two have larger deviations. So an appropriate choice of
parametric model seems to be somewhat important, contrary to ``common
knowledge'', which says that it is not so important to have a good fit of
the baseline hazard, if the only interest lies in the regression parameters.

\subsubsection{Testing the proportionality assumption with the Pch model}

One advantage with the piecewise constant hazards model is that it is easy
to test
the assumption of proportional hazards. But it cannot be done directly with
the {\tt phreg} function. There is, however, a simple alternative. Before
we show how to do it, we must show how to utilize Poisson regression when
analyzing the pch model. We do it by reanalyzing the birth intervals data.

First, the data set is split up after the {\tt cuts} in {\tt strata}. This
is done with the aid of the {\tt survSplit} function in the {\tt survival}
package.
<<splitsurv6>>=
f12 <- fert[fert$parity == 1, ]
f12$enter <- 0 # 0 expands to a vector
f12.split <- survSplit(f12, cut = 1:13, start = "enter",
                       end = "next.ivl", event = "event",
                       episode = "ivl")
head(f12.split)
@
To see better what happened, we {\tt sort} the new data frame by {\tt id}
and {\tt enter}.
<<sortt>>=
f12.split <- f12.split[order(f12.split$id, f12.split$enter), ]
head(f12.split)
@
We see that a new variable, {\tt ivl}, is created. It tells us which
interval we are looking at. You may notice that the variables {\tt enter}
and {\tt ivl} happens to be equal here; it is because our intervals start
by 0, 1, 2, \ldots, 13, as given by {\tt cuts} above.

In the Poisson regression to follow, we model the probability that {\tt
  event = 1}. This probability will depend on the length of the studied
interval, here {\tt next.ivl - enter}, which for our data mostly equals one.
The exact value we need is the logarithm of this difference, and it will be
entered as an {\tt offset}\index{offset} in the model.

Finally, one parameter per interval is needed, corresponding to the
piecewise constant hazards. This is accomplished by including {\tt ivl} as
a {\tt factor} in the model. Thus we get
<<poispch6>>=
f12.split$offs <- log(f12.split$next.ivl - f12.split$enter)
f12.split$ivl <- as.factor(f12.split$ivl)
fit12.pn <- glm(event ~ offset(offs) +
                age + year + ses + ivl,
                family = "poisson", data = f12.split)
drop1(fit12.pn, test = "Chisq")
@
The piecewise cutting ({\tt ivl}) is very statistically significant, but
some caution with the interpretation is recommended: The cut generated 13
parameters, and there may
be too few events in some intervals. This easily checked with the function
{\tt tapply}\index{Functions!\fun{tapply}}, which is very handy for doing calculations on
subsets of the data frame. In this case we want to sum the number of {\tt
  event}s in each {\tt ivl}:
<<sumivl6>>=
tapply(f12.split$event, f12.split$ivl, sum)
@
A reasonable interpretation of this is to restrict attention to the 10
first years; only one birth occurs after a longer waiting time. Then it
would be reasonable to collapse the intervals in $(7, 10]$ to one
interval. Thus
<<collapsebir6>>=
fc <- age.window(f12.split, c(0, 11),
                 surv = c("enter", "next.ivl", "event"))
levels(fc$ivl) <- c(0:6, rep("7-11", 7))
tapply(fc$event, fc$ivl, sum)
@
Note two things here. First the use of the function {\tt
  age.window}\index{Functions!\fun{age.window}}, second the {\tt levels} function. The
first makes an ``age cut'' in the Lexis diagram, i.e., all spells are
censored at (exact) age 11. The second is more intricate; factors can be
tricky to handle. The problem here is that after {\tt age.window}, the new
data frame contains only individuals with values {\tt 0, 1, \ldots, 11} on
the factor variable {\tt ivl}, but it is still \emp{defined} as a factor
with 14 levels. The call to {\tt levels} collapses the last seven to ``{\tt 7-11}''.

Now we rerun the analysis with the new data frame.
%\begin{table}
%\tabletitle{Yup}
%\label{tab:}
<<rerun0pch6>>=
fit <- glm(event ~  offset(offs) + age + year + ses + ivl,
           family = "poisson", data = fc)
drop1(fit, test = "Chisq")
@
%\end{table}
This change in categorization does not change the general conclusions about
statistical significance at all.

Now let us include interactions with {\tt ivl} in the model.
<<rerunpch6>>=
fit.ia <- glm(event ~  offset(offs) + (age + year + ses) * ivl,
              family = "poisson", data = fc)
drop1(fit.ia, test = "Chisq")
@
There is an apparent interaction with {\tt age}, which is not very
surprising; younger mothers will have a longer fertility period ahead than
old mothers. The other interactions do not seem to be very important, so we
remove them and rerun the model once again.
<<rerun2pch6>>=
fit.ia <- glm(event ~  offset(offs) + year + ses + age * ivl,
              family = "poisson", data = fc)
drop1(fit.ia, test = "Chisq")
@
Let us look at the parameter estimates.
<<rer6>>=
summary(fit.ia)
@
We can see (admittedly not clearly) a tendency of decreasing intensity in
the later intervals with higher age.

This can actually also be done with {\tt phreg} and the \emp{Weibull} model and
fixed {\tt shape} at one (i.e., an \emp{exponential} model).
<<rer0exp6>>=
fit.exp <- phreg(Surv(enter, next.ivl, event) ~  year + ses +
                 age * ivl, dist = "weibull", shape = 1,
                 data = fc)
drop1(fit.exp, test = "Chisq")
@
The parameter estimates are now presented like this.
<<rerexp6>>=
fit.exp
@
A lot of information, but the same content as in the output from the
Poisson regression. However, here we get the exponentiated parameter
estimates, in the column ``\emp{Exp(Coef)}''. These numbers are \emp{risk
    ratios} (or relative risks), and easier to interpret.

What should be done with the interactions? The simplest way to go to get an
easy-to-interpret result is to categorize {\tt age} and make separate
analyzes, one for each category
of {\tt age}. Let us do that; first we have to decide on how to
categorize: there should not be too many categories, and not too few
mothers (or births) in any category. As a starting point, take three
categories by cutting {\tt age} at (exact) ages 25 and 30:
<<cutage6>>=
fc$agegrp <- cut(fc$age, c(0, 25, 30, 100),
                 labels = c("< 25", "25-30", ">= 30"))
table(fc$agegrp)
@
Note the use of the function {\tt cut}. It "cuts" a continuous variable
into pieces, defined by the second argument. Note that we must give lower
and upper bounds; I chose 0 and 100, respectively. They are of course not
near real ages, but I am sure that no value falls outside the interval $(0,
100)$, and that is the important thing here. Then I can (optionally) give
names to each category with the argument {\tt labels}.

The tabulation shows that the oldest category contains relatively few
mothers. Adding to that, the oldest category will probably also have fewer
births. We can check that with the aid of the function {\tt tapply}.
<<tappcut6>>=
tapply(fc$event, fc$agegrp, sum)
@
Now, rerun the last analysis for each {\tt agegrp} separately. For
illustration only, we show how to do it for the first age group:

<<grpbir16>>=
fit.ses1 <- phreg(Surv(enter, next.ivl, event) ~  year + ses + ivl,
dist = "weibull", shape = 1, data = fc[fc$agegrp == "< 25", ])
fit.ses1
@
You may try to repeat this for all age groups and check if the regression
parameters for {\tt year} and {\tt ses} are very different. Hint: They are not.

\subsection{Choosing the best parametric model}
%\end{quote}

%\subsection{Parametric proportional hazards in \R}

For modeling survival data with parametric proportional hazards models,
the distributions of the function {\tt phreg} in the package {\tt eha} are
available. How to select a suitable parametric model is shown by a couple
of examples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Old age mortality}

Survival data from the \emp{Sundsvall region}

\includegraphics[height=3in]{figure/ddb.png}
\includegraphics[height=2.5in]{figure/sullan.jpg}

\end{frame}


\begin{frame}[fragile]{Old age mortality}

\scriptsize
<<oldage>>=
head(oldmort)
@

\end{frame}
\begin{frame}[fragile]{Summary}

\scriptsize
<<oldmort6>>=
summary(oldmort[, c(2:5, 8:13)])
@

\end{frame}

\begin{frame}[fragile]{Data Stucture (str)}
\scriptsize
<<struct>>=
str(oldmort)

@

\end{frame}

\begin{frame}[fragile]{Recoding, first fit}

\scriptsize

<<oldmort6.reg,results='asis'>>=
om <- oldmort
om$Y <- Surv(om$enter - 60, om$exit - 60, om$event)
fit.w <- phreg(Y ~ birthdate + sex + civ + ses.50 +
                   birthplace + imr.birth + region , data = om)
dr = drop1(fit.w, test = "Chisq")
xtable(dr)
@

\end{frame}

\end{document}



\begin{frame}[fragile]{Remove ses.50}

\scriptsize

<<oldmort6.reg2,results='asis'>>=
fit.w2 <- phreg(Y ~ birthdate + sex + civ + birthplace +
                    imr.birth + region , data = om)
dr2 = drop1(fit.w2, test = "Chisq")
xtable(dr2)
@

\end{frame}


\begin{frame}[fragile]{Remove ses.50 and imr.birth}

\scriptsize

<<oldmort6.reg3,results='asis'>>=
fit.w3 <- phreg(Y ~ birthdate + sex + civ + birthplace +
                    region , data = om)
dr3 = drop1(fit.w3, test = "Chisq")
xtable(dr3)
@

\end{frame}


\begin{frame}[fragile]{Remove ses.50, imr.birth, and birthplace}

\scriptsize

<<oldmort6.reg4,results='asis'>>=
fit.w4 <- phreg(Y ~ birthdate + sex + civ +
                    region , data = om)
dr4 = drop1(fit.w4, test = "Chisq")
xtable(dr4)
@

\end{frame}

\begin{frame}[fragile]{Tests of proportionality, cumulative hazards}

<<grtest, fig.height = 5, echo = FALSE>>=
par(mfrow = c(2, 2))
fit.sex <- phreg(Y ~ birthdate + strata(sex) + civ +
                    region , data = om)
plot(fit.sex, main = "sex", fn = "cum", col = 1:2)
fit.civ <- phreg(Y ~ birthdate + sex + strata(civ) +
                    region , data = om)
plot(fit.civ, main = "Civil status", fn = "cum", col = 1:4)
fit.reg <- phreg(Y ~ birthdate + sex + civ +
                    strata(region), data = om)
plot(fit.reg, main = "Region", fn = "cum", col = 1:2)
fit.all <- phreg(Y ~ birthdate + sex + civ +
                    region , data = om)
plot(fit.all, main = "All", fn = "cum")
@

\end{frame}

\end{document}

\begin{frame}[fragile]{Graphical tests of proportionality, hazards}

<<grtesthaz, fig.height = 5, echo = FALSE>>=
par(mfrow = c(2, 2))
fit.sex <- phreg(Y ~ birthdate + strata(sex) + civ +
                    region , data = om)
plot(fit.sex, main = "sex", fn = "haz", col = 1:2)
fit.civ <- phreg(Y ~ birthdate + sex + strata(civ) +
                    region , data = om)
plot(fit.civ, main = "Civil status", fn = "haz", col = 1:4)
fit.reg <- phreg(Y ~ birthdate + sex + civ +
                    strata(region), data = om)
plot(fit.reg, main = "Region", fn = "haz", col = 1:2)
fit.all <- phreg(Y ~ birthdate + sex + civ +
                    region , data = om)
plot(fit.all, main = "All", fn = "haz")
@

\end{frame}

\begin{frame}[fragile]{Tests of proportionality, nonparametric}

<<grtestnon, fig.height = 5, echo = FALSE>>=
oldpar = par(mfrow = c(2, 2))
fit.sex <- coxreg(Y ~ birthdate + strata(sex) + civ +
                    region , data = om)
plot(fit.sex, main = "sex", fn = "cum", col = 1:2)
fit.civ <- coxreg(Y ~ birthdate + sex + strata(civ) +
                    region , data = om)
plot(fit.civ, main = "Civil status", fn = "cum", col = 1:4)
fit.reg <- coxreg(Y ~ birthdate + sex + civ +
                    strata(region), data = om)
plot(fit.reg, main = "Region", fn = "cum", col = 1:2)
fit.allc <- coxreg(Y ~ birthdate + sex + civ +
                    region , data = om)
plot(fit.allc, main = "All", fn = "cum")
par(oldpar)
@

\end{frame}

\begin{frame}[fragile]{Check of the Weibull assumption}

<<checkdist>>=
check.dist(fit.allc, fit.all)

@
\end{frame}

Here we applied a \emp{\emp{Weibull}} baseline distribution (the {\tt default}
distribution in {\tt phreg}; by specifying nothing, the \emp{Weibull} is chosen).
Now let us repeat this with all the distributions in the {\tt phreg} package.
<<allreg6>>=
ln <- phreg(Y ~ sex + civ + birthplace, data = om,
            dist = "lognormal")
ll <- phreg(Y ~ sex + civ + birthplace, data = om,
            dist = "loglogistic")
g <- phreg(Y ~ sex + civ + birthplace, data = om,
           dist = "gompertz")
ev <- phreg(Y ~ sex + civ + birthplace, data = om,
            dist = "ev")
@
Then we compare the maximized log-likelihoods and choose the distribution with the
largest value.
<<compare6>>=
xx <- c(fit.w$loglik[2], ln$loglik[2], ll$loglik[2],
        g$loglik[2], ev$loglik[2])
names(xx) <- c("w", "ln", "ll", "g", "ev")
xx
@
The \emp{Gompertz} (g) distribution gives the largest value of the
maximized log likelihood. Let us graphically inspect the fit, see
Figure~\ref{fig:grafi6}.
\begin{figure}[ht!]
<<grainsp6>>=
fit.c <- coxreg(Y ~ sex + civ + birthplace, data = om)
check.dist(fit.c, g)
@
\caption[Check of a \emp{Gompertz} fit to old age mortality]{Graphical fit; \emp{Gompertz} baseline versus the nonparametric (Cox
  regression\index{Cox regression}, dashed).}
\label{fig:grafi6}
\end{figure}
The fit is obviously great during the first 30 years (ages 60--90), but
fails somewhat thereafter. One explanation to ``the failure'' is that after age
90, not so
many persons are still at risk (alive); another is that maybe the mortality
levels off at very high ages (on a very high level, of course).

The \emp{Gompertz} hazard function is shown in Figure~\ref{fig:olmgom6}.
\begin{figure}[ht!]
<<gomphaz6>>=
plot(g, fn = "haz")
@
\caption[Estimated \emp{Gompertz} hazard function for old age mortality]{Estimated \emp{Gompertz} hazard function for old age mortality data.}
\label{fig:olmgom6}
\end{figure}
It is an exponentially increasing function of time; the Gompertz model
usually fits old age mortality very well.

The $p$-values measure the significance of a group compared to the
reference category, but we would also like to have an overall $p$-value for
the covariates (factors) as such, i.e., answer the question ``does
birthplace matter?''. This can be achieved by running an analysis without
{\tt birthplace}:
<<without6>>=
fit.w1 <- phreg(Y ~ sex + civ, data = om)
fit.w1
@
%\end{quote}
Then we compare the \emp{max log likelihoods}:
\Sexpr{round(fit.w$loglik[2], digits = 3)} and
\Sexpr{round(fit.w1$loglik[2], digits = 3)}, respectively. The test
statistic is 2 times the difference,
\Sexpr{round(2 * (fit.w$loglik[2] - fit.w1$loglik[2]), digits = 3)}.
Under the null hypothesis of no {\tt birthplace} effect on mortality,
this test statistic has an approximate
$\chi^2$ distribution with 2 degrees of freedom. The degrees of freedom is
the number of omitted parameters in the reduced model, two in this
case. This because the factor {\tt birthplace} has three levels.

There is a much simpler way of doing this, and that is to use the function {\tt
  drop1}. As its name may suggest, it drops one variable at a time and
reruns the fit, calculating the max log likelihoods and differences as above.
<<drop6>>=
drop1(fit.w, test = "Chisq")
@
As you can see, we do not only recover the test statistic for {\tt
  birthplace}, we get the corresponding tests for all the involved
covariates. Thus, it is obvious that both {\tt sex} and {\tt civ} have a
statistically very significant  effect on old age mortality, while {\tt
  birthplace} does not mean much. Note that these effects are measured in
the presence of the other two variables.

We can plot the baseline distribution by
<<plotno6>>=
plot(fit.w)
@
with the result shown in Figure~\ref{fig:6weibas}.
\begin{figure}[ht!]
<<6weibas,echo=FALSE>>=
plot(fit.w)
@
\caption[Distribution of remaining life at 60, \emp{Weibull} model]{Baseline distribution of remaining life at 60 for an ``average''
  person. \emp{Weibull} model.}
\label{fig:6weibas}
\end{figure}
We can see one advantage with parametric models here: They make it
possible to estimate the hazard and density functions. This is much
trickier with a semi-parametric model like the Cox proportional hazards
model. Another question is, of course, how well the \emp{Weibull} model fits the
data. One way to graphically check this is to fit a Cox
regression\index{Cox regression} model
and compare the two cumulative hazards plots. This is done by using the
function {\tt check.dist}:\index{cumulative hazard function}
<<chekwei6>>=
fit <- coxreg(Y ~ sex + civ + birthplace, data = om)
check.dist(fit, fit.w)
@
The result is shown in Figure~\ref{fig:6chekwei}.
\begin{figure}[ht!]
<<cw6fig,echo=FALSE>>=
check.dist(fit, fit.w)
@
\caption[Check of a \emp{Weibull} fit to old age mortality]{Check of a \emp{Weibull} fit. The solid line is the cumulative hazards
  function from the \emp{Weibull} fit, and the dashed line is the fit from a Cox
  regression\index{Cox regression}.}
\label{fig:6chekwei}
\end{figure}
Obviously, the fit is not good; the \emp{Weibull} model cannot capture the fast
rise of the hazard by age. An exponentially increasing hazard function may
be needed, so let us
try the \emp{Gompertz} distribution:
<<gomp6>>=
fit.g <- phreg(Y ~ sex + civ + birthplace, data = om,
               dist = "gompertz")
fit.g
@
One sign of a much better fit is the larger value of the maximized log
likelihood, \Sexpr{round(fit.g$loglik[2], digits = 3)} versus
\Sexpr{round(fit.w$loglik[2], digits = 3)} for the \emp{Weibull} fit.
  The comparison to the Cox regression\index{Cox regression} fit is given by
<<chekgom6>>=
check.dist(fit, fit.g)
@
with the result shown in Figure~\ref{fig:6chekgom}.
\begin{figure}[ht!]
<<cg6fig,echo=FALSE>>=
check.dist(fit, fit.g)
@
\caption[Check of a \emp{Gompertz} fit to old age mortality]{Check of a
  \emp{Gompertz} fit. The solid line is the cumulative hazards
  function from the \emp{Gompertz} fit, and the dashed line is the fit from a Cox
  regression\index{Cox regression}.}
\label{fig:6chekgom}
\end{figure}
This is a much better fit. The deviation that starts above 30
(corresponding to the age 90) is no big issue; in that range few people are
still alive and the random variation takes over. Generally, it seems as if
the \emp{Gompertz} distribution fits old age mortality well.

The plots of the baseline \emp{Gompertz} distribution is shown in Figure~\ref{fig:6gombas}.
\begin{figure}[ht!]
<<6gombas,echo=FALSE,fig.height=6>>=
plot(fit.g)
@
\caption[Distribution of remaining life at 60, \emp{Gompertz} model]{Baseline distribution of remaining life at 60 for an ``average''
  person. \emp{Gompertz} model.}
\label{fig:6gombas}
\end{figure}
Compare to the corresponding fit to the \emp{Weibull} model, Figure~\ref{fig:6weibas}.



\section{Accelerated failure time models}

The accelerated failure time (AFT) model is best described through relations
between survivor functions. For instance,
comparing two groups:

\begin{description}
  \item[Group 0:] $P(T \ge t) = S_0(t)$  (control group)
    \item[Group 1:] $P(T \ge t) = S_0(\phi t)$ (treatment group)
\end{description}
The model says that treatment \emp{accelerates} failure time by the factor $\phi$.
If $\phi < 1$, treatment is good (prolongs life), otherwise bad.
Another interpretation is that the \emp{median} life length is \emp{
  multiplied by $1/\phi$} by treatment.

In Figure~\ref{fig:6aftph} the difference between the accelerated failure
time  and the
proportional hazards models concerning the hazard functions is illustrated.
\begin{figure}[ht!]
<<aftph6,echo=FALSE>>=
x <- seq(0, 3, length = 1000)
par(mfrow = c(1, 2))
plot(x, 2 * hllogis(x, shape = 5), type = "l", ylab = "", main = "PH", xlab = "Time")
lines(x, hllogis(x, shape = 5), lty = 2)
plot(x, 2 * hllogis(2 * x, shape = 5), type = "l", ylab = "", main = "AFT", xlab = "Time")
lines(x, hllogis(x, shape = 5), lty = 2)
@
\caption[Proportional hazards and accelerated failure time models]{Proportional hazards (left) and accelerated failure time model
  (right). The baseline distribution is Loglogistic with shape 5 (dashed).}
\label{fig:6aftph}
\end{figure}
The AFT hazard is not only multiplied by 2, it is also shifted to the left;
the process is accelerated. Note how the hazards in the AFT case converges
as time increases. This is usually a sign of the suitability of an AFT model.

\subsection{The AFT regression model}
\index{model!parametric!AFT|(}
If $T$ has survivor function $S(t)$ and $T_c = T/c$,  then $T_c$ has
survivor function $S(ct)$.
Then, if $Y = \log(T)$ and $Y_c = \log(T_c)$, the
following relation holds:
\begin{equation*}
Y_c = Y - log(c).
\end{equation*}
With $Y = \epsilon$, $Y_c = Y$, and $\log(c) = -\bbeta \bx$ this can be written in
familiar form:
\begin{equation*}
Y = \bbeta \bx + \epsilon,
\end{equation*}
i.e., an ordinary linear regression model for the log survival times. In
the absence of right censoring and left truncation, this model can be
estimated by least squares. However, the presence of these forms of
incomplete data makes it necessary to rely on maximum likelihood
methods. In \R, there is the functions {\tt aftreg} in the package {\tt
  eha} and the function {\tt survreg} in the package {\tt survival} that
perform the task of fitting AFT models.

Besides differing parametrizations, the main difference between {\tt
  aftreg} and {\tt survreg} is that the latter does not allow for left
truncated data. One reason for this is that left truncation is a much
harder problem to deal with in AFT models than in proportional hazards models.

Here we describe the implementation in {\tt aftreg}. A detailed description
of it can be found in Appendix~\ref{app:B}. The model is built around
\emp{scale-shape} families of distributions:
\begin{equation}\label{eq:6aftnull}
S^\star\bigl\{t, (\lambda, p)\bigr\} = S_0\biggl\{\biggl(\frac{t}{\lambda}\biggr)^p\biggr\}, \quad t > 0; \;
\lambda, p > 0,
\end{equation}
where $S_0$ is a fixed distribution. For instance, the exponential with
parameter~1:
\begin{equation*}
S_0(t) = e^{-t}, \quad t > 0,
\end{equation*}
generates, through \eqref{eq:6aftnull}, the \emp{Weibull} family
\eqref{eq:6weibull} of distributions.

%The allowance for left
%truncation and time-varying covariates forces the consideration of the
%extended model

\newcommand{\tl}{\biggl(\frac{t\exp(\bbeta\bx)}{\lambda}\biggr)^p}

With time-constant covariate vector $\bx$, the AFT model is

\begin{equation}
    S(t; (\lambda, p, \bbeta), \bx) = S^\star\bigl\{t\exp(\bbeta\bx),
    (\lambda, p)\bigr\} =
    S_0\biggl\{\biggl(\frac{t\exp(\bbeta\bx)}{\lambda}\biggr)^p\biggr\},
  \quad t > 0,
\end{equation}
and this leads to the following description of the hazard function:
\begin{multline*}
    h(t; (\lambda, p, \bbeta), \bx) = \exp(\bbeta\bx)
    h^\star\bigl(t\exp(\bbeta\bx)\bigr) = \\
    \exp(\bbeta\bx)\frac{p}{\lambda}
    \biggl(\frac{t\exp(\bbeta\bx)}{\lambda}\biggr)^{p-1}h_0\biggl\{\tl\biggr\},
    \quad t > 0,
\end{multline*}
where $\bx = (x_1, \ldots, x_p)$ is a vector of covariates, and
$\bbeta = (\beta_1, \ldots, \beta_p)$ is the corresponding
        vector of regression coefficients.
\index{model!parametric!AFT|)}

\subsection{Different parametrizations}
\index{Functions!\fun{aftreg}|(}
In \R, there are two functions that can estimate AFT regression models, the
function {\tt aftreg} in the package {\tt eha}, and the function {\tt
  survreg} in the package {\tt survival}. In the case of no time-varying
covariates and no left truncation, they fit the same models (given a common
baseline distribution), but use different parametrizations, which will be
explained here.

\subsection{AFT models in \R}
 We repeat the examples from the proportional hazards section, but with AFT
 models instead.

 \begin{quote} {Old age mortality} \end{quote}
 For a description of this data set, see above. Here we fit an AFT model
 with the \emp{Weibull} distribution. This should be compared to the proportional
 hazards model with the \emp{Weibull} distribution, see earlier in this chapter.
<<oldmort6.aft>>=
fit.w1 <- aftreg(Y ~ sex + civ + birthplace, data = om)
fit.w1
@

Note that the ``Max. log. likelihood'' is exactly the same, but the
regression parameter estimates differ. The explanation to this is that (i)
for the \emp{Weibull} distribution, the AFT and the PH models are the same, and
(ii) the only problem is that different parametrizations are used. The
$p$-values and the signs of the parameter estimates should be the same.
\eex
\index{Functions!\fun{aftreg}|)}


\section{Proportional hazards or AFT model?}

The problem of choosing between a proportional hazards and an accelerated
failure time model (everything else equal) can be solved by comparing the
AIC\index{AIC} of the models. Since the numbers of parameters are equal in
the two
cases, this amounts to comparing the maximized likelihoods. For instance,
in the case with \emp{old age mortality}:

Let us see what happens with the \emp{Gompertz} AFT model:
Exactly the same procedure as with the \emp{Weibull} distrbution, but we have to
specify the \emp{Gompertz} distribution in the call (remember, the \emp{Weibull}
distribution is the default choice, both for {\tt phreg} and {\tt aftreg}).
<<oldln6.aft>>=
fit.g1 <- aftreg(Y ~ sex + civ + birthplace, data = om,
                 dist = "gompertz")
fit.g1
@

Comparing the corresponding result for the proportional hazards and the AFT
models with the \emp{Gompertz} distribution,
we find that the maximized log likelihood in the former case is
\Sexpr{round(fit.g1$loglik[2], 3)}, compared to
\Sexpr{round(fit.g$loglik[2], 3)} for the latter. This indicates that the
proportional hazards model
  fit is better. Note however that we cannot formally test the proportional
  hazards hypothesis; the two models are not nested.

%\section{Piecewise constant hazards}

\section{Discrete time models}
There are two ways of looking at discrete duration data; either time is truly
discrete, i.e, the number of trials until an event occurs, or an
approximation due to rounding of continuous time data. In a sense all data
are discrete, because it is impossible to measure anything on a continuous
scale with infinite precision, but from a practical point of view it is
reasonable to say that data is discrete when tied events occur
embarrassingly often.

When working with register data, time is often measured in years which
makes it necessary and convenient to work with discrete models. A typical
data format is the so-called \emp{wide} format, where there is one record
(row) per individual, and measurements for many years. We have so far only
worked with the \emp{long} format. The data sets created by {\tt survSplit}
are in long format; there is one record per individual and age category.
The \R\ work horse in switching back and forth between the long and wide
formats is the function {\tt reshape}. It may look confusing at first, but
if data follow some simple rules, it is quite easy to use {\tt reshape}.

The function {\tt reshape} is typically used with \emp{longitudinal data},
where there are several measurements at different time points for each
individual. If the data for one individual is registered within one record
(row), we say that data are in wide format, and if there is one record (row)
per time (several records per individual), data are in long format. Using
wide format, the rule is that time-varying variable names must end in a
numeric value indicating at which time the measurement was taken. For
instance, if the variable {\tt civ} (civil status) is noted at times 1, 2,
and 3, there must be variables named {\tt civ.1, civ.2, civ.3},
respectively. It is optional to use any \emp{separator} between the base
name ({\tt civ}) and the time, but it should be one character or empty. The
``.'' is what {\tt reshape} expects by default, so using that form
simplifies coding somewhat.

We start by creating an example data set as an illustration. This is
accomplished by starting off with the data set {\tt oldmort} in {\tt eha}
and ``trimming'' it.
<<trimort6>>=
data(oldmort)
om <- oldmort[oldmort$enter == 60, ]
om <- age.window(om, c(60, 70))
om$m.id <- om$f.id <- om$imr.birth <- om$birthplace <- NULL
om$birthdate <- om$ses.50 <- NULL
om1 <- survSplit(om, cut = 61:69, start = "enter", end = "exit",
                 event = "event", episode = "agegrp")
om1$agegrp <- factor(om1$agegrp, labels = 60:69)
om1 <- om1[order(om1$id, om1$enter), ]
head(om1)
@
We may change the row numbers and recode the {\tt id} so they are easier to read.
<<recode6>>=
rownames(om1) <- 1:NROW(om1)
om1$id <- as.numeric(as.factor(om1$id))
head(om1)
@
This is the long format, each individual has as many records as "presence
ages". For instance, person No.\ 1 has four records, for the ages 60--63.
The maximum possible No.\ of records for one individual is 10. We can check
the distribution of No.\ of records per person by using the function {\tt
  tapply}:
<<recspp6>>=
recs <- tapply(om1$id, om1$id, length)
table(recs)
@
It is easier to get to grips with the distribution with a graph, in this
case a \emp{barplot}\index{Functions!\fun{barplot}},see Figure~\ref{fig:barp6}.
\begin{figure}[ht!]
<<barp6>>=
barplot(table(recs))
@
\caption{Barplot of the number of records per person.}
\label{fig:barp6}
\end{figure}

Now, let us turn {\tt om1} into a data frame in {\tt wide} format. This is
done with the function {\tt reshape}. First we remove the redundant
variables {\tt enter} and {\tt exit}.
<<wideform6>>=
om1$exit <- om1$enter <- NULL
om2 <- reshape(om1, v.names = c("event", "civ", "region"),
               idvar = "id", direction = "wide",
               timevar = "agegrp")
names(om2)
@
Here there are two time-fixed variables, {\tt id} and {\tt sex}, and three
time-varying variables, {\tt event}, {\tt civ}, and {\tt region}. The
time-varying variables have suffix of the type {\tt .xx}, where {\tt xx} varies
from 60 to 69.

This is how data in wide format usually show up; the suffix may start with
something else than {\tt .}, but it must be a single character, or nothing.
The real problem is how to switch from wide format to long, because our
survival analysis tools want it that way. The solution is to use {\tt
reshape} again, but other specifications.

<<reshap26>>=
om3 <- reshape(om2, direction = "long", idvar = "id",
               varying = 3:32)
head(om3)
@
There is a new variable {\tt time} created, which goes from 60 to 69, one
step for each of the ages. We would
like to have the file sorted primarily by {\tt id} and secondary by time.
<<sortin6>>=
om3 <- om3[order(om3$id, om3$time), ]
om3[1:11, ]
@
Note that all individuals got 10 records here, even those who only are
observed for fewer years. Individual No.\ 1 is only observed for the ages
60--63, and the next six records are redundant; they will not be used in an
analysis if kept, so it is from a practical point of view a good idea to
remove them.
<<remove6>>=
NROW(om3)
om3 <- om3[!is.na(om3$event), ]
NROW(om3)
@
The data frame shrunk to almost half of what it was originally. First, let
us summarize data.
<<summm6>>=
summary(om3)
@
The key variables in the discrete time analysis are {\tt event} and {\tt
  time}. For the baseline hazard we need one parameter per value of {\tt
  time}, so it is practical to transform the continuous variable {\tt time}
to a factor.
<<turn6>>=
om3$time <- as.factor(om3$time)
summary(om3)
@
The summary now produces a frequency table for {\tt time}.

For a given time point and a given individual, the response is whether an
event has occurred or not, i.e., it is modeled as a
\emp{Bernoulli}\index{Distributions!Bernoulli} outcome, which is a special
case of the \emp{binomial} distribution\index{Distributions!binomial}.
The discrete time analysis may now be performed in several ways. Most
straightforward is to run a
logistic regression\index{logistic regression} with {\tt event} as
response through the basic {\tt glm} function with
{\tt family = binomial(link=cloglog)}. The so-called
\emp{cloglog} link\index{cloglog link} is
used in order to preserve the proportional hazards property.
<<glmreg6>>=
fit.glm <- glm(event ~ sex + civ + region + time,
               family = binomial(link = cloglog), data = om3)
summary(fit.glm)
@
This output is not so pleasant, but we can anyway see that females (as
usual) have lower mortality than males, that married are better off than
unmarried, and that regional differences maybe are not so large. To get a
better understanding of the statistical significance of the findings we run
{\tt drop1}\index{Functions!\fun{drop1}} on the fit.
<<glmdrop6>>=
drop1(fit.glm, test = "Chisq")
@
Mildly surprisingly, {\tt civil status} is not that statistically
significant, but {\tt region} (and the other variables) is. The strong
significance of the time variable is of course expected; mortality is
expected to increase with higher age.

An equivalent way, with a nicer printed output, is to use the function
\fun{glmmboot}\index{Functions!\fun{glmmboot}} in the package {\tt eha}.
<<glmmboot6>>=
fit.boot <- glmmboot(event ~ sex + civ + region, cluster = time,
                     family = binomial(link = cloglog),
                     data = om3)
fit.boot
@
The parameter estimates corresponding to `{\tt time} are contained in the
variable {\tt fit\$frail}. They need to be transformed to get the "baseline
hazards".
<<calchaz6>>=
haz <- plogis(fit.boot$frail)
haz
@
A plot of the hazard function is shown in Figure~\ref{fig:bashaz6}.
\begin{figure}[ht!]
<<plothaz6>>=
barplot(haz)
@
\caption{Baseline hazards, old age mortality.}
\label{fig:bashaz6}
\end{figure}
% A smooted version is obtained by assuming a random effects model, where
% {\tt time} is the grouping factor.
% <<ML6>>=
% fit.ML <- glmmML(event ~ sex + civ + region, cluster = time, family = binomial(link = cloglog), data = om3)
% fit.ML
% haz <- fit.ML$posterior.modes
% haz
% @
% And the plot is, see Figure \ref{fig:bashazML6}.
% \begin{figure}[ht!]
% <<plothazML6,fig=TRUE>>=
% barplot(haz)
% @
% \caption{Baseline hazards, old age mortality.}
% \label{fig:bashazML6}
% \end{figure}
By some data manipulation we can also use {\tt eha} for the analysis. For
that to succeed we need intervals as responses, and the way of doing that
is to add two variables, \var{exit} and \var{enter}. The latter must be
\emp{slightly} smaller than the former:
<<mlreg6>>=
om3$exit <- as.numeric(as.character(om3$time))
om3$enter <- om3$exit - 0.5
fit.ML <- coxreg(Surv(enter, exit, event) ~ sex + civ + region,
                 method = "ml", data = om3)
fit.ML
@

Plots of the cumulative hazards and the survival function are easily
achieved, see Figures~\ref{fig:cumML6} and \ref{fig:surML6}.
\begin{figure}[ht!]
<<plotcum6>>=
plot(fit.ML, fn = "cum", xlim = c(60, 70))
@
\caption{The cumulative hazards, from the coxreg fit.}
\label{fig:cumML6}
\end{figure}

\begin{figure}[ht!]
<<plotsur6>>=
plot(fit.ML, fn = "surv", xlim = c(60, 70))
@
\caption{The survival function, from the coxreg fit.}
\label{fig:surML6}
\end{figure}
Finally, the proportional hazards assumption can be tested in the discrete
time framework by creating an interaction between {\tt time} and the
covariates in question. It is only possible by using {\tt glm}.
<<testph6>>=
fit2.glm <- glm(event ~ (sex + civ + region) * time,
                family = binomial(link = cloglog),
                data = om3)
drop1(fit2.glm, test = "Chisq")
@
There is no sign of non-proportionality.

\end{document}
