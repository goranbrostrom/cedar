\documentclass[a4paper]{beamer}

\usepackage[utf8]{inputenc}
\usepackage{url}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}

%\PassOptionsToPackage{pdf}{pstricks} %used for pdflatex
%\usepackage{pstricks,pst-plot,pst-node,pst-func}

\newcommand{\R}{{\bf R}}
\newcommand{\emp}[1]{\textcolor{blue}{#1}}
\newcommand{\fun}[1]{{\tt #1}}
\newcommand{\eex}{\vspace{\baselineskip}}
\newcommand{\var}[1]{{\tt #1}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bx}{\mathbf{x}}

\usetheme{Singapore}% best choice.
 \setbeamercovered{transparent}%

%\usepackage{natbib}

 <<include=FALSE>>=
library(knitr)
opts_chunk$set(
fig.path='figs/', fig.height = 4,cache=TRUE,cache.path="cache8/"
)
@

\title{Parametric models}

\subtitle{Demography and Event History using R \\ UmeÃ¥ 1--12 June 2015}
\date{June 8, 2015}

\begin{document}

\maketitle

\begin{frame}{Parametric models}

In \emp{\tt eha} three kinds of parametric models are available;
\begin{itemize}
\item the \emp{Proportional  Hazards} (\emp{PH}),
\item the \emp{Accelerated Failure
  Time} (\emp{AFT}) and
\item the \emp{discrete
  time proportional hazards}  models.
\end{itemize}

\end{frame}

\begin{frame}{Generating a proportional hazards family}

\begin{itemize}
\item  If \emp{$h_0$} is the hazard function
corresponding to the \emp{generating distribution},
\item  \emp{$h_1$} is a \emp{member of the family} if
\begin{equation*}
  \emp{h_1(t) = c h_0(t)} \quad \text{for some } c > 0, \; \text{and all } t > 0.
\end{equation*}

\end{itemize}

\emp{Note:} The resulting proportional hazards
class of distributions \emp{may or may not} be a \emp{well recognized family} of
distributions.

\end{frame}

\begin{frame}{Parametric distributions in eha}

The parametric distributions that can be used as the \emp{generating
distribution} in the function \emp{\tt phreg} are the

\begin{itemize}
\item \emp{Weibull},
\item \emp{Piecewise constant hazards} (\fun{``pch''}),
\item \emp{Extreme value},
\item \emp{Gompertz},
\item \emph{Lognormal},
\item \emph{Loglogistic}.
\end{itemize}

 The \emp{lognormal} and
\emp{loglogistic} distributions
allow for hazard functions that are first increasing to a maximum and then
decreasing.
\end{frame}

\begin{frame}[fragile]{Some examples}


<<6hazs,echo=FALSE,fig.height=5,message=FALSE>>=
require(eha)
oldpar <- par(mfrow = c(2, 2))
x <- seq(0, 10, length = 1000)
plot(x, hweibull(x, shape = 2, scale = 2), main = "Weibull, p = 2", type = "l",
     ylab = "", xlab = "Time")
plot(x, hweibull(x, shape = 1/2, scale = 2), main = "Weibull, p = 1/2", type = "l",
     ylab = "", xlab = "Time", ylim = c(0, 2))
plot(x, hlnorm(x, shape = 1, scale = 2), main = "Lognormal", type = "l",
     ylab = "", xlab = "Time")
y <- hgompertz(x, scale = 5, shape = 1)
plot(x, y, main = "Gompertz", type = "l",
     ylab = "", xlab = "Time", ylim = c(0, max(y)))
par(oldpar)
@

\end{frame}


\begin{frame}{Two disparate distribution choices}

\begin{itemize}
\item The \emp{Weibull} distribution,

\begin{itemize}
\item One reason for the popularity of
the \emp{Weibull} distribution is that the survivor and hazard functions
both have fairly simple closed forms, and the proportional hazards models
built from a \emp{Weibull} distribution stays within the family of \emp{Weibull}
distributions.
\end{itemize}

\item The \emp{Lognormal} distribution
\begin{itemize}
\item does not have
any of these nice properties. Is \emp{not} a popular choice among PH models.
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}[fragile]{The Weibull distribution}

\begin{equation*}
h(t; (p, \lambda)) = \frac{p}{\lambda}
\biggl(\frac{t}{\lambda}\biggr)^{p-1}, \; t > 0; \; p, \lambda > 0
\end{equation*}

is \emp{closed under PH}:

\begin{equation*}
c \times h(t; (p, \lambda))  = h(t; (p, \lambda^\prime))
\end{equation*}

with
\begin{equation*}
\lambda^\prime = \lambda c^{-\frac{1}{p}}
\end{equation*}

\begin{itemize}
\item for \emp{each fixed $p > 0$}, a \emp{PH family} is
generated by \emp{varying $\lambda$} (the \emp{scale}).
\item two members from the Weibull family,  with
\emp{different values of $p$} (the \emp{shape}), are \emp{not proportional}.
\end{itemize}

\end{frame}


\begin{frame}[fragile]{Weibull PH regression}


The \emp{PH regression} model with a \emp{Weibull} baseline
distribution as obtained by \emp{multiplying by $\exp(\bbeta \bx)$}:
\begin{equation*}
  h(t; \bx, \lambda, p, \bbeta) = \frac{p}{\lambda}
  \biggl(\frac{t}{\lambda}\biggr)^{p-1} e^{\bbeta \bx}, \quad t > 0.
\end{equation*}
The function \emp{\tt phreg} in package \emp{\tt eha} fits these models by default.

\end{frame}

\begin{frame}[fragile]{Crude cumulative hazards}

<<plotsex,fig.height=4>>=
with(oldmort, plot(Surv(enter, exit, event),
                   strata = sex, col = c("blue", "red"),
                   xlab = "Age"))
@

\end{frame}


\begin{frame}[fragile]{The data set, center covariates}
The data set {\tt fert}:
  birth intervals for married women in 19th century Skellefte{\aa}.
  Here only the
  intervals starting with the first birth for each woman are considered.

\scriptsize

<<fert6.12>>=
library(xtable)
require(eha)
data(fert)
f12 <- fert[fert$parity == 1, ]
f12$Y <- Surv(f12$next.ivl, f12$event)
f12$age <- f12$age - 25
f12$year <- f12$year - 1850
head(f12)
@

\end{frame}

\begin{frame}[fragile]{The 'Y' variable}

Some women never got a second child, for instance the first woman ({\tt id
  = 1}) above. Also, note the just created variable {\tt Y}; it is printed
as a vector, with some values having a trailing``$+$''; Those are the
censored observations ({\tt event = 0}). But {\tt Y} is really a matrix, in
this case with two columns. The first column equals {\tt next.ivl} and the
second is {\tt event}.
<<Y12>>=
is.matrix(f12$Y)
dim(f12$Y)
@

\end{frame}

\begin{frame}[fragile]{The Weibull fit}

<<phweib, results='asis'>>=
fit.w <- phreg(Y ~ age + year + ses, data = f12)
xtable(drop1(fit.w, test = "Chisq"))
@

\begin{equation*}
\text{AIC} = 2 \bigl(n - \max(\text{loglik})\bigr), \; n = \text{No.\ of parameters}
\end{equation*}

 (``Akaike Information Criterion'', 1973)

``Smaller is better!''
\end{frame}


\begin{frame}[fragile]{The Weibull fit, time to second birth}

\scriptsize

<<fitw,results='asis'>>=
ltx(fit.w)
kof <- fit.w$coef[1]
@

The estimated coefficient for {\tt age},
\Sexpr{round(kof, digits = 3)}, is negative, \ldots

\end{frame}


\begin{frame}[fragile]{The baseline hazard}
\scriptsize
<<weibfert6,fig.height=3>>=
oldpar <- par(mfrow = c(1, 2))
plot(fit.w, fn = "haz")
plot(fit.w, fn = "cum")
par(oldpar)
@

\end{frame}

\begin{frame}{The Lognormal model}

\begin{itemize}

\item $X$ is \emp{Lognormal} implies \emp{$Y = \log(X)$ is Normal}.


\item Both the
hazard and the survivor functions \emp{lack closed forms}.

\item \emp{Multiplying the hazard function by a constant} not equal to 1
leads to a \emp{non-lognormal} hazard function.

\item So, while we for the \emp{Weibull} family of distributions found subfamilies with
proportional hazards by keeping $p$ fixed and varying $\lambda$, for the
\emp{lognormal} distribution we cannot use the same trick.
\item By
multiplying the hazard function by a constant we arrive at a \emp{three-parameter
family} of distributions.

\end{itemize}

\end{frame}

\begin{frame}[fragile]{Lognormal fit}
\scriptsize
<<phlognorm,results='asis'>>=
fit.lognorm <- phreg(Y ~ age + year + ses, data = f12,
                     dist = "lognormal")
xtable(drop1(fit.lognorm, test = "Chisq"))
@

\end{frame}


\begin{frame}[fragile]{The Lognormal fit}
\scriptsize
<<fitlogist,results='asis'>>=
dr = drop1(fit.lognorm, test = "Chisq")
ltx(fit.lognorm, dr = dr, digits = 4)
@
<<secret16,echo=FALSE>>=
kof <- fit.lognorm$coef[1]
@
The estimated coefficient for {\tt age},
\Sexpr{round(kof, digits = 3)}, is negative, \ldots

\end{frame}

\begin{frame}[fragile]{The figure, Lognormal}
\scriptsize
<<lognormfert6,fig.height=3>>=
oldpar <- par(mfrow = c(1, 2))
plot(fit.lognorm, fn = "haz", main = "Hazard function")
plot(fit.lognorm, fn = "cum", main = "Cumulative hazard function")
par(oldpar)
@

\end{frame}


\begin{frame}{Comparing the Weibull and Lognormal fits}

\begin{itemize}
\item At least one of the fits must be less good (Why?)
\item There are two direct ways
of comparing the fits.
\begin{enumerate}
\item Look at the maximized log likelihoods. For
the \emp{Weibull} fit it is \Sexpr{round(fit.w$loglik[2], digits = 2)} and for
the \emp{lognormal} fit it is
\emp{\Sexpr{round(fit.lognorm$loglik[2], digits = 2)}}. The rule is that the
largest value wins, so the \emp{lognormal} model is the clear winner.
\begin{itemize}
\item Note that
this is \emp{not} a formal test; a likelihood ratio test
would require that the two models are \emp{nested}, but that
is not the case here. This is (here) equivalent to using the \emp{AIC}
as \emp{a
measure of comparison}, because the two models have the same number of
parameters.
\end{itemize}
\item Graphical: Plot the cumulative
hazard functions
against the nonparametric estimate from a Cox
regression
fit, and judge which looks closer.
\end{enumerate}
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Graphical check of fit}
\scriptsize
\begin{itemize}
\item Fit a Cox model with
the same covariates:
<<coxr6>>=
fit.cr <- coxreg(Y ~ age + year + ses, data = f12)
@

\item Then the function {\tt check.dist} (from {\tt eha}) is called. The \emp{Weibull} fit:

<<drycompw,fig.height=4>>=
check.dist(fit.cr, fit.w)
@

\end{itemize}

This fit looks very poor.
\end{frame}


\begin{frame}[fragile]{Shorter follow-up time}

\scriptsize
Note below that after calling {\tt age.window},
$Y$ \emp{must} be recreated! This is the danger with this ``lazy'' approach.
<<trunc6f12cens,results='asis'>>=
f12$enter <- rep(0, NROW(f12))
f12.cens <- age.window(f12, c(0, 12),
                       surv = c("enter", "next.ivl", "event"))
f12.cens$Y <- Surv(f12.cens$enter, f12.cens$next.ivl,
                   f12.cens$event)
fit.wc <- phreg(Y ~ age + year + ses, data = f12.cens)
fit.c <- coxreg(Y ~ age + year + ses, data = f12.cens)
ltx(fit.wc)
@
\end{frame}

\begin{frame}[fragile]{Graphical check, Weibull, 0--12 years}

<<drycompwcens>>=
check.dist(fit.c, fit.wc)
@

This wasn't much better.

\end{frame}

\begin{frame}[fragile]{The lognormal model}
\scriptsize
<<trunc6f12cens.ln,results='asis'>>=
fit.lnc <- phreg(Y ~ age + year + ses, data = f12.cens,
                 dist = "lognormal")
dr <- drop1(fit.lnc, test = "Chisq")
ltx(fit.lnc, dr = dr)
@

\end{frame}

\begin{frame}[fragile]{Graphical check, Lognormal}

<<drycomplncens>>=
check.dist(fit.c, fit.lnc)
@

\end{frame}

\begin{frame}[fragile]{Why do we get so lousy fits?}

\begin{itemize}
\item This distribution has
features that the ordinary parametric distributions cannot cope with: Its
hazard function starts off \emp{being zero for almost one year}.
\item There is a positive probability of not getting a second child.
\item Maybe try a \emp{cure} model? Something like

\begin{equation*}
P(T > t) = P(\text{infertile}) + P(\text{fertile}) P(T > t \mid
  \text{fertile})
\end{equation*}

\end{itemize}

\end{frame}



\begin{frame}[fragile]{The piecewise constant hazards (pch) model}
\scriptsize
<<pch6fert, results='asis'>>=
fit.pch <- phreg(Surv(next.ivl, event) ~ age + year + ses,
                 data = f12, dist = "pch", cuts = c(4, 8, 12))
fit.c <- coxreg(Surv(next.ivl, event) ~ age + year + ses, 
                data = f12)
dr = drop1(fit.pch, test = "Chisq")
ltx(fit.pch, dr = dr)
@

\end{frame}

\begin{frame}[fragile]{Graphical check of the fit}

<<drycomppchcens>>=
check.dist(fit.c, fit.pch)
@

This is not good enough.
\end{frame}

\begin{frame}[fragile]{Tighter cuts}
\scriptsize
 <<pchfert2,results='asis'>>=
fit.pch <- phreg(Surv(next.ivl, event) ~ age + year + ses,
                 data = f12, dist = "pch", cuts = 1:13)
dr = drop1(fit.pch, test = 'Chisq')
ltx(fit.pch, dr = dr)
@
This gives us 14 intervals with constant baseline hazard, i.e., 14
parameters extra.
\end{frame}

\begin{frame}[fragile]{The graph}
<<drycomppch13cens>>=
check.dist(fit.c, fit.pch)
@

\end{frame}

\begin{frame}[fragile]{Plotting the hazard function}

<<pchhaz6>>=
plot(fit.pch, fn = "haz", main = "")
@

\end{frame}

\begin{frame}[fragile]{Sensitivity of regression parameter estimates}

<<threecomp6,echo=FALSE>>=
##require(xtable)
##fit.c$coef
##fit.w$coef
##fit.lognorm$coef
##fit.pch$coef
ta <- cbind(fit.c$coef[1:5], fit.pch$coef[1:5], fit.lognorm$coef[2:6], fit.w$coef[1:5])
colnames(ta) <- c("Cox", "Pch", "Lognormal", "Weibull")
ta <- round(exp(ta), digits = 3)
print(ta)
@
\begin{itemize}
\item The differences are \emp{not large}. 
\item The \emp{Cox} and the \emp{Pch} models are very \emp{close}.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{The proportionality assumption with the Pch model}
\scriptsize
 The data set is split up after the {\tt cuts} in {\tt strata}. This
is done with the aid of the {\tt survSplit} function in the {\tt survival}
package.
<<splitsurv6>>=
f12 <- fert[fert$parity == 1, ]
f12$enter <- 0 # 0 expands to a vector
f12.split <- survSplit(f12, cut = 1:13, start = "enter",
                       end = "next.ivl", event = "event",
                       episode = "ivl")
head(f12.split)
@
\end{frame}

\begin{frame}[fragile]{Sort the result}
  
\scriptsize
To see better what happened, we {\tt sort} the new data frame by {\tt id}
and {\tt enter}.
<<sortt>>=
f12.split <- f12.split[order(f12.split$id, f12.split$enter), ]
head(f12.split)
@
\begin{itemize}
\item A a new variable, {\tt ivl}, is created. It tells us which
interval we are looking at. You may notice that the variables {\tt enter}
and {\tt ivl} happens to be equal here; it is because our intervals start
by 0, 1, 2, \ldots, 13.
\item Interval lengths are {\tt next.ivl - enter}.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Adjusting the data frame for Poisson regression}
\scriptsize  
<<poispch6>>=
f12.split$offs <- log(f12.split$next.ivl - f12.split$enter)
f12.split$ivl <- as.factor(f12.split$ivl)
fit12.pn <- glm(event ~ offset(offs) +
                age + year + ses + ivl,
                family = "poisson", data = f12.split)
drop1(fit12.pn, test = "Chisq")
@

\end{frame}

\begin{frame}[fragile]{Interaction with age}
<<intactage>>=
tapply(f12.split$event, f12.split$ivl, sum)
@
A reasonable interpretation of this is to restrict attention to the 10
first years; only one birth occurs after a longer waiting time. Then it
would be reasonable to collapse the intervals in $(7, 10]$ to one
interval. Thus
\end{frame}

\begin{frame}[fragile]{Collapse late intervals}
<<collapsebir6>>=
fc <- age.window(f12.split, c(0, 11),
                 surv = c("enter", "next.ivl", "event"))
levels(fc$ivl) <- c(0:6, rep("7-11", 7))
tapply(fc$event, fc$ivl, sum)
@

\end{frame}

\begin{frame}[fragile]{Rerun}
  
\scriptsize  
<<rerun0pch6>>=
fit <- glm(event ~  offset(offs) + age + year + ses + ivl,
           family = "poisson", data = fc)
drop1(fit, test = "Chisq")
@

This change in categorization does not change the general conclusions about
statistical significance at all.
\end{frame}

\begin{frame}[fragile]{The interactions}
\scriptsize
<<rerunpch6>>=
fit.ia <- glm(event ~  offset(offs) + (age + year + ses) * ivl,
              family = "poisson", data = fc)
drop1(fit.ia, test = "Chisq")
@

\end{frame}

%%\end{document}

%% There is an apparent interaction with {\tt age}, which is not very
%% surprising; younger mothers will have a longer fertility period ahead than
%% old mothers. The other interactions do not seem to be very important, so we
%% remove them and rerun the model once again.
%% <<rerun2pch6>>=
%% fit.ia <- glm(event ~  offset(offs) + year + ses + age * ivl,
%%               family = "poisson", data = fc)
%% drop1(fit.ia, test = "Chisq")
%% @
%% Let us look at the parameter estimates.
%% <<rer6>>=
%% summary(fit.ia)
%% @
%% We can see (admittedly not clearly) a tendency of decreasing intensity in
%% the later intervals with higher age.

%This can actually also be done with {\tt phreg} and the \emp{Weibull} model and
%fixed {\tt shape} at one (i.e., an \emp{exponential} model).

%% \begin{frame}[fragile]{Do it with the phreg function}
%% \scriptsize
%% <<rer0exp6>>=
%% fit.exp <- phreg(Surv(enter, next.ivl, event) ~  year + ses +
%%                  age + ivl, dist = "weibull", shape = 1, data = fc)
%% drop1(fit.exp, test = "Chisq")
%% @
%% The parameter estimates are now presented like this.
%% <<rerexp6>>=
%% fit.exp
%% @

%% \end{frame}

%% \end{document}
%% A lot of information, but the same content as in the output from the
%% Poisson regression. However, here we get the exponentiated parameter
%% estimates, in the column ``\emp{Exp(Coef)}''. These numbers are \emp{risk
%%     ratios} (or relative risks), and easier to interpret.

%% What should be done with the interactions? The simplest way to go to get an
%% easy-to-interpret result is to categorize {\tt age} and make separate
%% analyzes, one for each category
%% of {\tt age}. Let us do that; first we have to decide on how to
%% categorize: there should not be too many categories, and not too few
%% mothers (or births) in any category. As a starting point, take three
%% categories by cutting {\tt age} at (exact) ages 25 and 30:
%% <<cutage6>>=
%% fc$agegrp <- cut(fc$age, c(0, 25, 30, 100),
%%                  labels = c("< 25", "25-30", ">= 30"))
%% table(fc$agegrp)
%% @
%% Note the use of the function {\tt cut}. It "cuts" a continuous variable
%% into pieces, defined by the second argument. Note that we must give lower
%% and upper bounds; I chose 0 and 100, respectively. They are of course not
%% near real ages, but I am sure that no value falls outside the interval $(0,
%% 100)$, and that is the important thing here. Then I can (optionally) give
%% names to each category with the argument {\tt labels}.

%% The tabulation shows that the oldest category contains relatively few
%% mothers. Adding to that, the oldest category will probably also have fewer
%% births. We can check that with the aid of the function {\tt tapply}.
%% <<tappcut6>>=
%% tapply(fc$event, fc$agegrp, sum)
%% @
%% Now, rerun the last analysis for each {\tt agegrp} separately. For
%% illustration only, we show how to do it for the first age group:

%% <<grpbir16>>=
%% fit.ses1 <- phreg(Surv(enter, next.ivl, event) ~  year + ses + ivl,
%% dist = "weibull", shape = 1, data = fc[fc$agegrp == "< 25", ])
%% fit.ses1
%% @
%% You may try to repeat this for all age groups and check if the regression
%% parameters for {\tt year} and {\tt ses} are very different. Hint: They are not.

\begin{frame}[fragile]{Choosing the best parametric model}

For modeling survival data with parametric proportional hazards models,
the distributions of the function {\tt phreg} in the package {\tt eha} are
available. How to select a suitable parametric model is shown by a couple
of examples.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% \begin{frame}{Old age mortality}

%% Survival data from the \emp{Sundsvall region}

%% \includegraphics[height=3in]{figure/ddb.png}
%% \includegraphics[height=2.5in]{figure/sullan.jpg}

%% \end{frame}


\begin{frame}[fragile]{Old age mortality}

\scriptsize
<<oldage>>=
head(oldmort)
@

\end{frame}
\begin{frame}[fragile]{Summary}

\scriptsize
<<oldmort6>>=
summary(oldmort[, c(2:5, 8:13)])
@

\end{frame}

\begin{frame}[fragile]{Data Stucture (str)}
\scriptsize
<<struct>>=
str(oldmort)

@

\end{frame}

\begin{frame}[fragile]{Recoding, first fit}

\scriptsize

<<oldmort6.reg,results='asis'>>=
om <- oldmort
om$Y <- Surv(om$enter - 60, om$exit - 60, om$event)
om$birthdate <- om$birthdate - 1830
fit.w <- phreg(Y ~ birthdate + sex + civ + ses.50 +
                   birthplace + imr.birth + region , data = om)
dr = drop1(fit.w, test = "Chisq")
xtable(dr)
@

\end{frame}



\begin{frame}[fragile]{Remove ses.50}

\scriptsize

<<oldmort6.reg2,results='asis'>>=
fit.w2 <- phreg(Y ~ birthdate + sex + civ + birthplace +
                    imr.birth + region , data = om)
dr2 = drop1(fit.w2, test = "Chisq")
xtable(dr2)
@

\end{frame}


\begin{frame}[fragile]{Remove ses.50 and imr.birth}

\scriptsize

<<oldmort6.reg3,results='asis'>>=
fit.w3 <- phreg(Y ~ birthdate + sex + civ + birthplace +
                    region , data = om)
dr3 = drop1(fit.w3, test = "Chisq")
xtable(dr3)
@

\end{frame}


\begin{frame}[fragile]{Remove ses.50, imr.birth, and birthplace}

\scriptsize

<<oldmort6.reg4,results='asis'>>=
fit.w4 <- phreg(Y ~ birthdate + sex + civ +
                    region , data = om)
dr4 = drop1(fit.w4, test = "Chisq")
xtable(dr4)
@

\end{frame}

\begin{frame}[fragile]{Tests of proportionality, cumulative hazards}

<<grtest, fig.height = 5, echo = FALSE>>=
oldpar <- par(mfrow = c(2, 2))
fit.sex <- phreg(Y ~ birthdate + strata(sex) + civ +
                    region , data = om)
plot(fit.sex, main = "sex", fn = "cum", col = 1:2)
fit.civ <- phreg(Y ~ birthdate + sex + strata(civ) +
                    region , data = om)
plot(fit.civ, main = "Civil status", fn = "cum", col = c(1:2, 4))
fit.reg <- phreg(Y ~ birthdate + sex + civ +
                    strata(region), data = om)
plot(fit.reg, main = "Region", fn = "cum", col = c(1:2, 4))
fit.all <- phreg(Y ~ birthdate + sex + civ +
                    region , data = om)
plot(fit.all, main = "All", fn = "cum")
par(oldpar)
@

\end{frame}

\begin{frame}[fragile]{Graphical tests of proportionality, hazards}

<<grtesthaz, fig.height = 5, echo = FALSE>>=
par(mfrow = c(2, 2))
fit.sex <- phreg(Y ~ birthdate + strata(sex) + civ +
                    region , data = om)
plot(fit.sex, main = "sex", fn = "haz", col = 1:2)
fit.civ <- phreg(Y ~ birthdate + sex + strata(civ) +
                    region , data = om)
plot(fit.civ, main = "Civil status", fn = "haz", col = c(1:2, 4))
fit.reg <- phreg(Y ~ birthdate + sex + civ +
                    strata(region), data = om)
plot(fit.reg, main = "Region", fn = "haz", col = c(1:2, 4))
fit.all <- phreg(Y ~ birthdate + sex + civ +
                    region , data = om)
plot(fit.all, main = "All", fn = "haz")
@

\end{frame}

\begin{frame}[fragile]{Tests of proportionality, nonparametric}

<<grtestnon, fig.height = 5, echo = FALSE>>=
oldpar = par(mfrow = c(2, 2))
fit.sex <- coxreg(Y ~ birthdate + strata(sex) + civ +
                    region , data = om)
plot(fit.sex, main = "sex", fn = "cum", col = 1:2)
fit.civ <- coxreg(Y ~ birthdate + sex + strata(civ) +
                    region , data = om)
plot(fit.civ, main = "Civil status", fn = "cum", col = 1:4)
fit.reg <- coxreg(Y ~ birthdate + sex + civ +
                    strata(region), data = om)
plot(fit.reg, main = "Region", fn = "cum", col = 1:2)
fit.allc <- coxreg(Y ~ birthdate + sex + civ +
                    region , data = om)
plot(fit.allc, main = "All", fn = "cum")
par(oldpar)
@

\end{frame}

\begin{frame}[fragile]{Check of the Weibull assumption}

<<checkdist>>=
check.dist(fit.allc, fit.all)

@
\end{frame}

\begin{frame}[fragile]{Other distributions}
\scriptsize
<<allreg6>>=
ln <- phreg(Y ~ sex + civ + birthplace, data = om,
            dist = "lognormal")
ll <- phreg(Y ~ sex + civ + birthplace, data = om,
            dist = "loglogistic")
g <- phreg(Y ~ sex + civ + birthplace, data = om,
           dist = "gompertz")
ev <- phreg(Y ~ sex + civ + birthplace, data = om,
            dist = "ev")
@
Then we compare the maximized log-likelihoods and choose the distribution with the
largest value.
<<compare6>>=
xx <- c(fit.w$loglik[2], ln$loglik[2], ll$loglik[2],
        g$loglik[2], ev$loglik[2])
names(xx) <- c("w", "ln", "ll", "g", "ev")
xx
@
\end{frame}

\begin{frame}[fragile]{Check the Gompertz distribution}
\scriptsize  
The \emp{Gompertz} (g) distribution gives the largest value of the
maximized log likelihood.

<<grainsp6>>=
fit.c <- coxreg(Y ~ sex + civ + birthplace, data = om)
check.dist(fit.c, g, col = c("blue", "red"))
abline(h = 0)
@
\end{frame}

\begin{frame}[fragile]{The Gompertz hazard function}
<<gomphaz6>>=
plot(g, fn = "haz", col = "blue")
abline(h = 0)
@
\end{frame}

%% \end{document}

%% The $p$-values measure the significance of a group compared to the
%% reference category, but we would also like to have an overall $p$-value for
%% the covariates (factors) as such, i.e., answer the question ``does
%% birthplace matter?''. This can be achieved by running an analysis without
%% {\tt birthplace}:
%% <<without6>>=
%% fit.w1 <- phreg(Y ~ sex + civ, data = om)
%% fit.w1
%% @
%% %\end{quote}
%% Then we compare the \emp{max log likelihoods}:
%% \Sexpr{round(fit.w$loglik[2], digits = 3)} and
%% \Sexpr{round(fit.w1$loglik[2], digits = 3)}, respectively. The test
%% statistic is 2 times the difference,
%% \Sexpr{round(2 * (fit.w$loglik[2] - fit.w1$loglik[2]), digits = 3)}.
%% Under the null hypothesis of no {\tt birthplace} effect on mortality,
%% this test statistic has an approximate
%% $\chi^2$ distribution with 2 degrees of freedom. The degrees of freedom is
%% the number of omitted parameters in the reduced model, two in this
%% case. This because the factor {\tt birthplace} has three levels.

%% There is a much simpler way of doing this, and that is to use the function {\tt
%%   drop1}. As its name may suggest, it drops one variable at a time and
%% reruns the fit, calculating the max log likelihoods and differences as above.
%% <<drop6>>=
%% drop1(fit.w, test = "Chisq")
%% @
%% As you can see, we do not only recover the test statistic for {\tt
%%   birthplace}, we get the corresponding tests for all the involved
%% covariates. Thus, it is obvious that both {\tt sex} and {\tt civ} have a
%% statistically very significant  effect on old age mortality, while {\tt
%%   birthplace} does not mean much. Note that these effects are measured in
%% the presence of the other two variables.

%% We can plot the baseline distribution by
%% <<plotno6>>=
%% plot(fit.w)
%% @
%% with the result shown in Figure~\ref{fig:6weibas}.
%% \begin{figure}[ht!]
%% <<6weibas,echo=FALSE>>=
%% plot(fit.w)
%% @
%% \caption[Distribution of remaining life at 60, \emp{Weibull} model]{Baseline distribution of remaining life at 60 for an ``average''
%%   person. \emp{Weibull} model.}
%% \label{fig:6weibas}
%% \end{figure}
%% We can see one advantage with parametric models here: They make it
%% possible to estimate the hazard and density functions. This is much
%% trickier with a semi-parametric model like the Cox proportional hazards
%% model. Another question is, of course, how well the \emp{Weibull} model fits the
%% data. One way to graphically check this is to fit a Cox
%% regression\index{Cox regression} model
%% and compare the two cumulative hazards plots. This is done by using the
%% function {\tt check.dist}:\index{cumulative hazard function}
%% <<chekwei6>>=
%% fit <- coxreg(Y ~ sex + civ + birthplace, data = om)
%% check.dist(fit, fit.w)
%% @
%% The result is shown in Figure~\ref{fig:6chekwei}.
%% \begin{figure}[ht!]
%% <<cw6fig,echo=FALSE>>=
%% check.dist(fit, fit.w)
%% @
%% \caption[Check of a \emp{Weibull} fit to old age mortality]{Check of a \emp{Weibull} fit. The solid line is the cumulative hazards
%%   function from the \emp{Weibull} fit, and the dashed line is the fit from a Cox
%%   regression\index{Cox regression}.}
%% \label{fig:6chekwei}
%% \end{figure}
%% Obviously, the fit is not good; the \emp{Weibull} model cannot capture the fast
%% rise of the hazard by age. An exponentially increasing hazard function may
%% be needed, so let us
%% try the \emp{Gompertz} distribution:
%% <<gomp6>>=
%% fit.g <- phreg(Y ~ sex + civ + birthplace, data = om,
%%                dist = "gompertz")
%% fit.g
%% @
%% One sign of a much better fit is the larger value of the maximized log
%% likelihood, \Sexpr{round(fit.g$loglik[2], digits = 3)} versus
%% \Sexpr{round(fit.w$loglik[2], digits = 3)} for the \emp{Weibull} fit.
%%   The comparison to the Cox regression\index{Cox regression} fit is given by
%% <<chekgom6>>=
%% check.dist(fit, fit.g)
%% @
%% with the result shown in Figure~\ref{fig:6chekgom}.
%% \begin{figure}[ht!]
%% <<cg6fig,echo=FALSE>>=
%% check.dist(fit, fit.g)
%% @
%% \caption[Check of a \emp{Gompertz} fit to old age mortality]{Check of a
%%   \emp{Gompertz} fit. The solid line is the cumulative hazards
%%   function from the \emp{Gompertz} fit, and the dashed line is the fit from a Cox
%%   regression\index{Cox regression}.}
%% \label{fig:6chekgom}
%% \end{figure}
%% This is a much better fit. The deviation that starts above 30
%% (corresponding to the age 90) is no big issue; in that range few people are
%% still alive and the random variation takes over. Generally, it seems as if
%% the \emp{Gompertz} distribution fits old age mortality well.

%% The plots of the baseline \emp{Gompertz} distribution is shown in Figure~\ref{fig:6gombas}.
%% \begin{figure}[ht!]
%% <<6gombas,echo=FALSE,fig.height=6>>=
%% plot(fit.g)
%% @
%% \caption[Distribution of remaining life at 60, \emp{Gompertz} model]{Baseline distribution of remaining life at 60 for an ``average''
%%   person. \emp{Gompertz} model.}
%% \label{fig:6gombas}
%% \end{figure}
%% Compare to the corresponding fit to the \emp{Weibull} model, Figure~\ref{fig:6weibas}.



\begin{frame}{Accelerated Failure Time (AFT) models}

\begin{description}
  \item[Group 0:] $P(T \ge t) = S_0(t)$  (control group)
    \item[Group 1:] $P(T \ge t) = S_0(\phi t)$ (treatment group)
\end{description}
The model says that treatment \emp{accelerates} failure time by the factor $\phi$.
If $\phi < 1$, treatment is good (prolongs life), otherwise bad.
Another interpretation is that the \emp{median} life length is \emp{
  multiplied by $1/\phi$} by treatment.
\end{frame}

\begin{frame}[fragile]{AFT vs.\ PH hazard functions}
  
<<aftph6,echo=FALSE>>=
x <- seq(0, 3, length = 1000)
par(mfrow = c(1, 2))
plot(x, 2 * hllogis(x, shape = 5), type = "l", ylab = "", main = "PH", xlab = "Time")
lines(x, hllogis(x, shape = 5), lty = 2)
plot(x, 2 * hllogis(2 * x, shape = 5), type = "l", ylab = "", main = "AFT", xlab = "Time")
lines(x, hllogis(x, shape = 5), lty = 2)
@

The AFT hazard is not only multiplied by 2, it is also shifted to the left;
the process is \emp{accelerated}. Note how the hazards in the AFT case
\emp{converges} 
as time increases. 

\end{frame}

\begin{frame}[fragile]{The AFT hazard function}
  
Survivor function:  
\begin{equation*}
S(t) = S_0(\phi t)
\end{equation*}
Density function:
\begin{equation*}
f(t) = \phi f_0(\phi t)
\end{equation*}
Hazard function:
\begin{equation*}
h(t) = \frac{f(t)}{S(t)} = \phi h_0(\phi t)
\end{equation*}

\end{frame}

\begin{frame}{The AFT regression model}

If $T$ has survivor function $S(t)$ and $T_c = T/c$,  then $T_c$ has
survivor function $S(ct)$.
Then, if $Y = \log(T)$ and $Y_c = \log(T_c)$, the
following relation holds:
\begin{equation*}
Y_c = Y - log(c).
\end{equation*}
With $Y = \epsilon$, $Y_c = Y$, and $\log(c) = -\bbeta \bx$ this can be written in
familiar form:
\begin{equation*}
Y = \bbeta \bx + \epsilon,
\end{equation*}

\emp{linear  regression}!

\emp{Complication:} Right censoring and left truncation!
\end{frame}

\begin{frame}[fragile]{AFT regression in R}
  
\begin{itemize}  
\item \emp{\tt aftreg} in the package {\tt eha}:
  \begin{itemize}
    \item Handles \emp{left truncation}
    \item Distributions: {\tt ``weibull''} (default), {\tt ``gompertz''},
      {\tt ``ev''}, {\tt ``lognormal''}, and  {\tt ``loglogistic''}.
   \end{itemize}   
\item \emp{\tt survreg} in the package {\tt survival}.
  \begin{itemize}
    \item Can \emp{not} be used with left truncation
      \item Can handle user-defined distributions in addition to built-in ones.
   \end{itemize}
\item \emp{Other} packages, see the \emp{CRAN Taskview}.   
\end{itemize}   

\end{frame}

\begin{frame}[fragile]{The {\tt aftreg} function}

Built around \emp{scale-shape} families of distributions:
\begin{equation*}
S^\star\bigl\{t, (\lambda, p)\bigr\} = S_0\biggl\{\biggl(\frac{t}{\lambda}\biggr)^p\biggr\}, \quad t > 0; \;
\lambda, p > 0,
\end{equation*}
where $S_0$ is a fixed distribution. 

\end{frame}


\begin{frame}[fragile]{Example: the Weibull family}
   
The \emp{exponential} with parameter 1:
\emp{$S_0(t) = e^{-t}, \; t > 0$},
generates the \emp{Weibull} family
of distributions:

\begin{equation*}
S\bigl(t; (p, \lambda)\bigr) =
\exp\biggl\{-\biggl(\frac{t}{\lambda}\biggr)^p\biggr\}, \; t > 0; \; p,
\lambda > 0
\end{equation*}  

\end{frame}

\begin{frame}[fragile]{The AFT regression model}
  
\newcommand{\tl}{\biggl(\frac{t\exp(\bbeta\bx)}{\lambda}\biggr)^p}

With time-constant covariate vector $\bx$, the AFT model is

\begin{equation*}
    S(t; (\lambda, p, \bbeta), \bx) = %%S^\star\bigl\{t\exp(\bbeta\bx),
    %%(\lambda, p)\bigr\} =
    S_0\biggl\{\biggl(\frac{t\exp(\bbeta\bx)}{\lambda}\biggr)^p\biggr\},
  \quad t > 0,
\end{equation*}
and this leads to the following description of the hazard function:
\begin{multline*}
    h(t; (\lambda, p, \bbeta), \bx) = \\ %%= \exp(\bbeta\bx)
    %%h^\star\bigl(t\exp(\bbeta\bx)\bigr) = \\
    \exp(\bbeta\bx)\frac{p}{\lambda}
    \biggl(\frac{t\exp(\bbeta\bx)}{\lambda}\biggr)^{p-1}h_0\biggl\{\tl\biggr\},
    \quad t > 0,
\end{multline*}
%where $\bx = (x_1, \ldots, x_p)$ is a vector of covariates, and
%$\bbeta = (\beta_1, \ldots, \beta_p)$ is the corresponding
 %       vector of regression coefficients.

\end{frame}

%\end{document}

%% \begin{frame}[fragile]{Different parametrizations}

%% In \R, there are two functions that can estimate AFT regression models, the
%% function {\tt aftreg} in the package {\tt eha}, and the function {\tt
%%   survreg} in the package {\tt survival}. In the case of no time-varying
%% covariates and no left truncation, they fit the same models (given a common
%% baseline distribution), but use different parametrizations, which will be
%% explained here.

\begin{frame}[fragile]{Fertility, Lognormal AFT}
\scriptsize  
<<lnaft,results='asis'>>=
fit.lna <- aftreg(Y ~ age + year + ses, data = f12.cens,
                  dist = "lognormal")
dr = drop1(fit.lna, test = "Chisq")
ltx(fit.lna, dr = dr)
@   

\end{frame}

\begin{frame}[fragile]{Fertility, Lognormal AFT, ``lifeExp''}
\scriptsize  
<<lnaftexp,results='asis'>>=
fit.lnae <- aftreg(Y ~ age + year + ses, data = f12.cens,
                  param = 'lifeExp', dist = "lognormal")
dre = drop1(fit.lnae, test = "Chisq")
ltx(fit.lnae, dr = dre)
@   

\end{frame}

\begin{frame}[fragile]{Fertility, Loglogistic AFT, ``lifeExp''}
\scriptsize  
<<lnaftexpgomp,results='asis'>>=
fit.lnaeg <- aftreg(Y ~ age + year + ses, data = f12.cens,
                  param = 'lifeExp', dist = "loglogistic")
dreg = drop1(fit.lnaeg, test = "Chisq")
ltx(fit.lnaeg, dr = dreg)
@   

\end{frame}

\begin{frame}[fragile]{The baseline hazard function, loglogistic}
  
<<basesur>>=
plot(fit.lnaeg, fn = "haz")
@   

\end{frame}

\begin{frame}[fragile]{Old age mortality}
\scriptsize
<<oldmort6.aft,results='asis'>>=
fit.w1 <- aftreg(Y ~ sex + civ + birthplace, data = om)
dr <- drop1(fit.w1, test = "Chisq")
ltx(fit.w1, dr = dr)
@
\end{frame}

\begin{frame}{Comparison PH vs.\ AFT}
\begin{itemize}
\item The ``Max. log. likelihood'' is exactly the same, but 
\item  the
regression parameter estimates differ. 
\item The explanation to this is that 
  \begin{enumerate}
    \item[(i)]
for the \emp{Weibull} distribution (only), the \emp{AFT} and the \emp{PH} models
are the same, and 
\item[(ii)] the only problem is that \emp{different parametrizations} are used. 
\end{enumerate}
\end{itemize}
\end{frame}


\begin{frame}[fragile]{PH or AFT model?}

\begin{itemize}  
\item Compare the
\emp{AIC} of the models. 
\item Since the numbers of parameters are equal in
the two
cases, this amounts to comparing the maximized likelihoods.
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Example: The Gompertz distribution}
  
<<oldln6.aft>>=
fit.ph <- phreg(Y ~ sex + civ + birthplace, data = om,
               dist = "gompertz")
fit.ph$loglik
fit.aft <- aftreg(Y ~ sex + civ + birthplace, data = om,
                 dist = "gompertz")
fit.aft$loglik
@

So, the \emp{PH} model is the winner (no formal test!).
\end{frame}

\begin{frame}[fragile]{The AFT fit with the Gompertz distribution}
\scriptsize 
<<gomfitph,results='asis'>>=
dr <- drop1(fit.aft, test = "Chisq")
ltx(fit.aft, test = "Chisq")
@   
\end{frame}

\begin{frame}[fragile]{Expected life length}

<<expect>>=
shape <- exp(fit.aft$coefficients["log(shape)"])
scale <- exp(fit.aft$coefficients["log(scale)"])
(expected <- integrate(pgompertz, 0, Inf, shape = shape, 
                       scale = scale, lower.tail = FALSE, 
                       param = "canonical"))
@ 
\end{frame}

\begin{frame}[fragile]{Calculating ``expected life''}
  
<<calexpl>>=
integrate(pgompertz, 0, Inf, scale = 9.224, 
          shape = 0.205, param = "canonical", 
          lower.tail = FALSE)
@

General result: \emp{$E(T) = \int_0^\infty S_T(t) dt$}, if $T$ is a lifetime.
\end{frame}

\begin{frame}[fragile]{Discrete time models}

<<trimort6,echo=FALSE>>=
data(oldmort)
om <- oldmort[oldmort$enter == 60, ]
om <- age.window(om, c(60, 70))
om$m.id <- om$f.id <- om$imr.birth <- om$birthplace <- NULL
om$birthdate <- om$ses.50 <- NULL
om1 <- survSplit(om, cut = 61:69, start = "enter", end = "exit",
                 event = "event", episode = "agegrp")
om1$agegrp <- factor(om1$agegrp, labels = 60:69)
#head(om1)
@
This is a "remake" of the {\tt oldmort} data set:
<<recode6,echo=FALSE>>=
om1$enter <- om1$exit <- NULL
om1$event <- as.logical(om1$event)
om1 <- om1[order(om1$id, om1$agegrp), ]
rownames(om1) <- 1:NROW(om1)
om1 <- om1[, c(1, 6, 2, 3:5)]
om1$id <- as.numeric(as.factor(om1$id))
head(om1)
@

\end{frame}

\begin{frame}[fragile]{Statistics}

<<statistics>>=
with(om1, table(agegrp, event))
@   
\end{frame}

\begin{frame}[fragile]{Logistic regression}
\scriptsize
<<logreg,results='asis'>>=
fit <- glm(event ~ sex + civ + region + agegrp, 
           data = om1, family = binomial(link = "cloglog"))
xtable(summary(fit)$coef, digits = 4)
@ 
\end{frame}

\begin{frame}[fragile]{With coxreg}
\scriptsize  
<<withcoxreg,results='asis'>>=
om1$age <- as.numeric(as.character(om1$age))
fit1 <- coxreg(Surv(age - 0.5, age, event) ~ sex + civ + region, 
               data = om1, method = "ml")
dr <- drop1(fit1, test = "Chisq")
ltx(fit1, dr = dr, digits = 4)
@ 

\end{frame}

\begin{frame}[fragile]{Plot of survivor function}
<<plotta>>=
plot(fit1, fn = "surv")
@ 
\end{frame}  

\begin{frame}[fragile]{Plot of hazards}
<<plotta2>>=
barplot(fit1$hazards[[1]][, 2], 
        names.arg = fit1$hazards[[1]][, 1], xlab = "age")
@ 
\end{frame}  
\end{document}


This is the long format, each individual has as many records as "presence
ages". For instance, person No.\ 1 has four records, for the ages 60--63.
The maximum possible No.\ of records for one individual is 10. We can check
the distribution of No.\ of records per person by using the function {\tt
  tapply}:
<<recspp6>>=
recs <- tapply(om1$id, om1$id, length)
table(recs)
@
It is easier to get to grips with the distribution with a graph, in this
case a \emp{barplot}\index{Functions!\fun{barplot}},see Figure~\ref{fig:barp6}.
\begin{figure}[ht!]
<<barp6>>=
barplot(table(recs))
@
\caption{Barplot of the number of records per person.}
\label{fig:barp6}
\end{figure}

Now, let us turn {\tt om1} into a data frame in {\tt wide} format. This is
done with the function {\tt reshape}. First we remove the redundant
variables {\tt enter} and {\tt exit}.
<<wideform6>>=
om1$exit <- om1$enter <- NULL
om2 <- reshape(om1, v.names = c("event", "civ", "region"),
               idvar = "id", direction = "wide",
               timevar = "agegrp")
names(om2)
@
Here there are two time-fixed variables, {\tt id} and {\tt sex}, and three
time-varying variables, {\tt event}, {\tt civ}, and {\tt region}. The
time-varying variables have suffix of the type {\tt .xx}, where {\tt xx} varies
from 60 to 69.

This is how data in wide format usually show up; the suffix may start with
something else than {\tt .}, but it must be a single character, or nothing.
The real problem is how to switch from wide format to long, because our
survival analysis tools want it that way. The solution is to use {\tt
reshape} again, but other specifications.

<<reshap26>>=
om3 <- reshape(om2, direction = "long", idvar = "id",
               varying = 3:32)
head(om3)
@
There is a new variable {\tt time} created, which goes from 60 to 69, one
step for each of the ages. We would
like to have the file sorted primarily by {\tt id} and secondary by time.
<<sortin6>>=
om3 <- om3[order(om3$id, om3$time), ]
om3[1:11, ]
@
Note that all individuals got 10 records here, even those who only are
observed for fewer years. Individual No.\ 1 is only observed for the ages
60--63, and the next six records are redundant; they will not be used in an
analysis if kept, so it is from a practical point of view a good idea to
remove them.
<<remove6>>=
NROW(om3)
om3 <- om3[!is.na(om3$event), ]
NROW(om3)
@
The data frame shrunk to almost half of what it was originally. First, let
us summarize data.
<<summm6>>=
summary(om3)
@
The key variables in the discrete time analysis are {\tt event} and {\tt
  time}. For the baseline hazard we need one parameter per value of {\tt
  time}, so it is practical to transform the continuous variable {\tt time}
to a factor.
<<turn6>>=
om3$time <- as.factor(om3$time)
summary(om3)
@
The summary now produces a frequency table for {\tt time}.

For a given time point and a given individual, the response is whether an
event has occurred or not, i.e., it is modeled as a
\emp{Bernoulli}\index{Distributions!Bernoulli} outcome, which is a special
case of the \emp{binomial} distribution\index{Distributions!binomial}.
The discrete time analysis may now be performed in several ways. Most
straightforward is to run a
logistic regression\index{logistic regression} with {\tt event} as
response through the basic {\tt glm} function with
{\tt family = binomial(link=cloglog)}. The so-called
\emp{cloglog} link\index{cloglog link} is
used in order to preserve the proportional hazards property.
<<glmreg6>>=
fit.glm <- glm(event ~ sex + civ + region + time,
               family = binomial(link = cloglog), data = om3)
summary(fit.glm)
@
This output is not so pleasant, but we can anyway see that females (as
usual) have lower mortality than males, that married are better off than
unmarried, and that regional differences maybe are not so large. To get a
better understanding of the statistical significance of the findings we run
{\tt drop1}\index{Functions!\fun{drop1}} on the fit.
<<glmdrop6>>=
drop1(fit.glm, test = "Chisq")
@
Mildly surprisingly, {\tt civil status} is not that statistically
significant, but {\tt region} (and the other variables) is. The strong
significance of the time variable is of course expected; mortality is
expected to increase with higher age.

An equivalent way, with a nicer printed output, is to use the function
\fun{glmmboot}\index{Functions!\fun{glmmboot}} in the package {\tt eha}.
<<glmmboot6>>=
fit.boot <- glmmboot(event ~ sex + civ + region, cluster = time,
                     family = binomial(link = cloglog),
                     data = om3)
fit.boot
@
The parameter estimates corresponding to `{\tt time} are contained in the
variable {\tt fit\$frail}. They need to be transformed to get the "baseline
hazards".
<<calchaz6>>=
haz <- plogis(fit.boot$frail)
haz
@
A plot of the hazard function is shown in Figure~\ref{fig:bashaz6}.
\begin{figure}[ht!]
<<plothaz6>>=
barplot(haz)
@
\caption{Baseline hazards, old age mortality.}
\label{fig:bashaz6}
\end{figure}
% A smooted version is obtained by assuming a random effects model, where
% {\tt time} is the grouping factor.
% <<ML6>>=
% fit.ML <- glmmML(event ~ sex + civ + region, cluster = time, family = binomial(link = cloglog), data = om3)
% fit.ML
% haz <- fit.ML$posterior.modes
% haz
% @
% And the plot is, see Figure \ref{fig:bashazML6}.
% \begin{figure}[ht!]
% <<plothazML6,fig=TRUE>>=
% barplot(haz)
% @
% \caption{Baseline hazards, old age mortality.}
% \label{fig:bashazML6}
% \end{figure}
By some data manipulation we can also use {\tt eha} for the analysis. For
that to succeed we need intervals as responses, and the way of doing that
is to add two variables, \var{exit} and \var{enter}. The latter must be
\emp{slightly} smaller than the former:
<<mlreg6>>=
om3$exit <- as.numeric(as.character(om3$time))
om3$enter <- om3$exit - 0.5
fit.ML <- coxreg(Surv(enter, exit, event) ~ sex + civ + region,
                 method = "ml", data = om3)
fit.ML
@

Plots of the cumulative hazards and the survival function are easily
achieved, see Figures~\ref{fig:cumML6} and \ref{fig:surML6}.
\begin{figure}[ht!]
<<plotcum6>>=
plot(fit.ML, fn = "cum", xlim = c(60, 70))
@
\caption{The cumulative hazards, from the coxreg fit.}
\label{fig:cumML6}
\end{figure}

\begin{figure}[ht!]
<<plotsur6>>=
plot(fit.ML, fn = "surv", xlim = c(60, 70))
@
\caption{The survival function, from the coxreg fit.}
\label{fig:surML6}
\end{figure}
Finally, the proportional hazards assumption can be tested in the discrete
time framework by creating an interaction between {\tt time} and the
covariates in question. It is only possible by using {\tt glm}.
<<testph6>>=
fit2.glm <- glm(event ~ (sex + civ + region) * time,
                family = binomial(link = cloglog),
                data = om3)
drop1(fit2.glm, test = "Chisq")
@
There is no sign of non-proportionality.

\end{document}
